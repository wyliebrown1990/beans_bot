 Welcome everyone. Welcome to the Observe hands-on workshop. My name is Lucas. I am one of the sales engineers here on the Observe team. Today, I'll be guiding you through the troubleshooting workflows slash Observe 101 and how to use the platform I'm joined by Kaya Champlin, one of our product managers here as well, and he'll be facilitating some of the content through the chat. If you have any questions as we go through this, feel free to throw them in the chat. Kaya will be there to help us out, send some links, documentation, and so on. So let's get started. Cool. Now, before we actually hop into it, a little inspirational quote from our host, Ron Grants-Wonsen here, the secret of success is to do the common thing, uncommonly well by John D. Rockefeller. So here at Observe, we do have this philosophy that everyone's trying to do observability and some people are doing it. All right, some people are trying their best. But we do think that we found sort of a really magic formula here in terms of how we're tackling the problems in this industry today. So as we go through this workshop, you know, keep this phrase in mind just to get us bumped up for the session that's coming up. One cool thing that will provide you after you complete this workshop is a little certificate signed by our CEO, Jeremy Barton. Quick little fun fact about Jeremy is that he's on the board for the McLaren Formula One team. So for a big F1 fan, you know, you can go there and show your co-workers his signature, whatever floats your boat there. But yeah, in terms of the agenda, quick breakdown here. The first 10 minutes, I'm going to have you guys create a trial account with an Observe so we can really can navigate through this process together. So if you can, if you could throw this link in the chat for people to click on it, go to htpsaccount.observe.com. So you can create your account as I go through the rest of the agenda. So go through this little presentation here. We're going to do a quick overview. On the actual workshop, we're going to talk about the architecture behind observe and some of the benefits that come along with it. And how we're differentiated in the market from other ability solutions. I'm going to introduce you guys some terminology behind observe, namely data streams, data sets, hopefully you're going to be hearing these words a lot. They really tie in everything together of what we're doing here in our texture. And then we're going to go through an overview of the actual environment, right, the hands on over environment, which is really your free trial. All the data that we're seeing here is going to be based off of the open fly, which is trying to be shop application. Right, so we're getting a lot of good metrics from from there. And we're going to do a troubleshooting process on top of that, right. Now for the remainder of the time, we're going to be doing the hands-on exercise. So the actual exercise is going to be a troubleshooting, containerized service. I'm going to have some nice quiz questions for you guys as we go along. To make sure that we're really putting the knowledge that we're gaining into use. We're going to do some data shaping 101 towards the end, which is going to be a bit of a challenge where we're going to use Opal, right, our processing language to go ahead and transform some data in some meaningful ways that we can use later on. And towards the end, we'll have plenty of time for Q&A. Anything you guys have any questions, anything that's not quite clear, anything you want to learn more about will be here for you to answer those questions. So with that in mind, let's talk about our texture, right. And I have just a quick sort of, I think it's important to use this diagram here just to illustrate how observe is different than in other solutions in the market, right. And the way I like to describe observe everyone is that we were the first observability solution that was built upon a data lake, right. The data lake in this case being snowflake. And we'd like to split sort of the understanding of this into two parts, data in-just part and data transformation part. So if you follow with me here on the left hand side of my screen, notice that observe ingests to let me show you from all your major cloud providers, all your major tools, whether it be host-based containerized Kubernetes, AWS, GCP, Azure. One of the things that we pride ourselves in is that we rely pretty heavily on open source technologies, right. We our team saw where the industry was going in terms of ingest and we realized that open source and the avoidance of vendor lock in is really the future of observability. So we use things like Prameepis, Grafana, Open-Toliner-Tree to facilitate that. Because of our data later texture and because of the separation of storage from compute, we're able to provide you 13 months retention and all of your data by default. And all of your data, really that comes into data lake, there's those trick ingest schemas on that, right. So if you guys want to send us structured unstructured JSON, plaintext, there's really no sort of convention issue how you want to send the data to us because all of the transformation is then post ingest. And the last thing I'll mention here in terms of the data that's coming in, we're not necessarily married to just observability data as well, right. As a matter of fact, you're observable, believe that observability is more of a data problem than just a telemetry problem. So we expect you to be sending us things like CRM information, right. You're ticketing information. You want to send us sort of jira's end-ass information you can, business contextual data, right. That we can then correlate to your telemetry and give you a full visibility into your actual application. So now that we've talked about data ingest, then we do our data transformation, right. And we'll see a lot of this throughout the following slides here. But just as a quick high level, this is what we call our data graph, right. Our data graph is split into what we call data sets. And those all are originated from what we call data streams. We're going to go into more detail on this, but I just want you guys to keep in mind that your data comes in into what we call our amount of machine data. So let's say amount of Kubernetes data, amount of AWS data. And then we shape that data into the meaningful views that are useful for your team. So for example, in the case of Kubernetes, we take all of your Kubernetes data and then we shape it into a container data set, a pod data set, right. A logs data set for all of the information that's coming from Kubernetes that's more meaningful to you. And we do this automatically. But let's dive a little bit deeper into those that terminology, right. That we that I've been mentioning here. So let's start off with data streams, right. Which is really the beginning point of where your data is coming from. So as you're seeing here on this slide, observe collects all data, system and application logs metrics and traces into observations, which are associated to specific data streams. You can imagine it's a source of data. Data streams are a flexible way to manage data ingestion. And each data stream sends raw observations to a source data set, right. Meaning that for each source of data that you have, we're going to have a raw data set that has all your data in it, as we'll see here in just a second. But then we've been talking about data sets, right. And data sets are derived from data streams. And they're structured with times or time intervals, as well as relations linking to or from other data sets. All right. So as we can see here on the right hand side of my screen, because you see that all of the data sets are interconnected because they're derived from one another going all the way back to that data stream or that raw data set that we're talking about. Now a data set has a schema, which is a set of named columns and definitions of data types and stored in those columns. All right. So as we take a look into data sets, you'll see that they look very familiar to what a SQL table would look like. Right. And so it should be something that you're used to seeing. But we offer a lot of flexibility on top of that. The last thing I'll mention here is that notice that there's two different colors for these data sets. Purple data sets is what we call an event data set. Right. And those events have timestamps or time intervals attached to them as I just discussed. On the other hand, blue data sets, they're tracking the state of an object through time. Right. And that object could be anything from a pod to a user to a customer to a node to a Damon set. Right. Things that are constant through time who state can change. The last thing I'll talk about is Opal. I've been sort of alluding to Opal, which is our observed frosting and analysis language. Now Opal is a query language for searching and transforming data within observe. It is a streaming language that allows you to both shape and query your data using the same underlying functions and verbs. It is purpose built to handle high volume times here's data and works across logs metrics traces and anything else you send us. Whether that be telemetry or business contextual data as I talked about before. Now one thing I'll mention here is that you might thinking, Oh, Lucas, you know, another query language. The beauty behind Opal is that everything that you do in the UI as you'll see. It's transformed into Opal in the backend. So if you want to be a regular user of observe who doesn't really dabble in Opal all that much, you can. Well, at the same time, if you want to have someone be a power user of observe, and they can really get deep into Opal and learn all of the sort of on teachers that that sit behind it. Right. So I just leave that with you guys. You'll see as we go along, everything with you in the UI is transforming to Opal in the backend. So on the environment itself, right. If you guys haven't yet created your your free trials, please go ahead to that link that kind of put in the chat here for you to create a trial so we can so you can follow along with us. And once again, everything's based off of the open telemetry demo. One thing that I'll say is that all the data that they will be dealing with is going to be namespace to demo data. Just to make it easier to follow and and locating things that being said, if you guys want to explore the platform and go off the beaten path. I feel free to do it. We're not we're not tied to just the data that we're looking at here. We want you to explore and learn as much as you can. Now, if you want to interact, right, I'll sent the the link to the application there. So you guys can go to the URL and an interactive website itself. Once again, all of the data is based off of the open telemetry data. And we'll be collecting logs, metrics and traces from it via our Kubernetes and old tell integrations. Right. Double click on it once again, everything would be namespace to demo data. But feel free to go off the beaten path if you want. On the workshop itself, just get a little bit deeper on the agenda is we're going to be walking through how to get familiar with the explorers within observe, which is what we call our unopenedated views. We're going to be using logs, metrics and traces to triage a user reported issue in this case. And then from there, we're going to build a dashboard sort of keep our investigation in one place and put all the context that we learned there into it. Right. We're going to be creating a monitor as well based on raw memory metrics so that we can sort of pinpoint this issue a little bit better later on. And then we're going to quickly build a new data set at the end to enrich our logs. And that's where we're going to learn about worksheets. And once again, add that back to our dashboard. Awesome. So with all of that information, all those housekeeping items out of the way. Let's. Let's jump in so just move my zoom window around here a little bit just make it easier on me. Hopefully everyone has their own. There's one more light slide here for us. Hopefully everyone has their trials up by now. That's the thank you slide spoiler alert. Let's hop into the actual environment. Hopefully everyone has a trial account by now. If you don't, if you have any questions about that, please. Through for the throat in the chat. I'm called will be more than glad to help you guys out. Awesome. So folks, everyone, welcome to observe. This will be the the welcome screen, but. I don't want to spend too much time on here. I actually want to get right into it. And the first place I want to introduce you guys to is the data set graph right will have been alluding to of how everything is interconnected with an observed. So as you hop into your trial, make sure we go into the data set graph piece here. And click on this little lineage view for us to get a good understanding right of what observed looks like from a 50 foot thousand view. So this is the graph for everything is interconnected. You should all the data sets are derived off of each other. As I mentioned before, we're going to be working off of the demo data name space. So please go ahead and make sure we filter out to just demo data so we can see all that a little bit closer here. Right. So right off the bat, you can see that we provide a number of ready to use apps that sort of model common infrastructure and application environments right things like AWS as you see down here things as. Kubernetes as you see in our data sort of namespace here things like open telemetry right for tracing and spans and application metrics. And as I mentioned in the terminology discussion demo data is the data stream, but it also exposes what we call our source data set right so what we've gone ahead here and done is use the Kubernetes and hotel integration. So then derive these materialized views of important context behind hotel and Kubernetes right things like metrics log traces pod health container health and so on. Now we'll get into the data graph a little bit more later on for now let's talk about today scenario right your mission should you choose to accept it in this case. So for today what we'll be doing is we're going to be putting on the hat of an SRE for a digital business, which is our astronomy shop and ecommerce website. And our team is responsible for ensuring the availability of the shop and responding both to regular on call cases, as well as any pages if your customer success team know gets reports that the application is going down or any potential issues. Today just happens to be one of those days you've been receiving a lot of problem reports from customers and they're indicating that the new important you may also like feature might be broken and when I say you may also like I mean like a recommendation feature right you bought a table you might also like a chair. No alerts have been going off yet, which is something that we must fix right because if there's issues going on we should probably have alerts on top of that. But we're going to do some initial triage with an observe anyways right so let's go ahead and get familiar with one of the first observed explorers that we're going to touch upon which is our logs explorer. So on the top left of your screen go ahead and click on to our logs explorer here. So we can gain view into all the logs that we have within our platform right now. So just to give you guys a quick sort of way of the land on what we're looking at here the way observe works and it looks like it's pretty similar to other solutions right if you've had to grab your way through issues. This sort of you should be pretty familiar to you. So on the top left inside of your logs explorer is where you choose the source of the data that you're looking at right in this case make sure we're looking at container logs coming from the demo data namespace so we're all looking into the same context here we can figure out what's what's wrong with our with our application. Now on the bottom of that we see filters right which really just map to the name of our columns right are sort of a tabular view as I mentioned before so we can go in filter into the values of those columns. So for example if I want to look into the logs coming from let's say the scarred service container I can click on that and you see that the context box plates right there right and then a couple of seconds here we have all that context in mind. So just to showcase that. And then we have ways of filtering into every single column as you see there. Now as you selected filters you can see that it's sort of update the updated that so this dynamic green builder and let's make sure that we're looking into our hotel or demo data namespace right our hotel demo namespace just make sure we're looking on the same data. And a couple of things that I want to showcase you guys here is that when we're filtering for specific information within our logs. I was going to do its best to sort of provide you a common values to search for right so in this case as I went into the space you sort of recommended me looking to all tell them because there's a lot of logs within that name space right and it just helps you out of complete. It also gives you sort of a summary of the possible operations that you can do with your logs here including things like substring search and so on. The one last thing that I'll mention for us here is that we're not sort of tied to this that we'll review either we can dive into what we call this chart view and that's important because it's going to take us to our first question of the day. The first question and the reason I've taken you guys to this sort of chart view is as a little hint is how many containers in the hotel demo namespace are sending logs to astroshop in the last 60 minutes. I'll give you guys one more hint we can choose a type of function that we're looking at right from here. And we can also choose the type of visualization that we're looking at right from here right so instead of a line chart that look at it as table single stat and so on. So let you guys I give you guys 30 seconds or so to go ahead and. Get that out maybe maybe 45 seconds maybe a minute Kyle are you able to throw the question out to them the question oh that is my bad I totally was typing to you not. Alright so the first quiz question as you go through it and we'll give you about a minute to answer this is as Lucas said sort of how many containers are in the hotel demo namespace. And just another bonus hit yeah show you can everything in observe is time series and so this idea of sort of defaulting to sort of a timeline views what we do we were pretty proud of that but. Summary is also something you may want to attempt to do. Just as it heads up so we're going to give you 30 more seconds but the question and answer is a polar quiz in zoom so you just go in when it pops up and click the answer to it as you see fit. And if you don't feel comfortable answering that's okay to it's okay to. So okay to. Looking strong nice i'm not going to say that is correct but it might be the most correct. It's definitely the least wrong it's the least wrong it's a good way of. There's actually some inside inside baseball there because things are time series the sort of rising and falling edge of things from sure folks have dealt this before you can sometimes go a little bit above a little bit below that's why we say if you do a summary that will be the sort of actual concrete. Summary every time. All right i'm going to end it go ahead and show them how it's done look yes. Perfect appreciate it Kyle so everyone there's multiple ways we could have gone about this and i'm sure you found your your respective ways you guys did get the correct answer 15 containers. How would you arrive at that right a couple of different ways as I said so for one already namespace sold out demo that's great. We can change the visualization type to be a single staff right which by default what I'll do is give us a single career value but in this case we're looking at a count of all events right what I really want. One is a count of distinct containers. Right and that should take us to that 15 number that we saw before now once again we could have used this you know we could have done this as a table we could have done this as a flying chart but the answer is 15 so good job everyone who got it right. Perfect how do I close this this quiz question now there we go it's gone. Awesome awesome so as some bonus points here we can also take a look at the you know as I mentioned before I just to bring that into view. Anything you do in the UI within observe is starting to opal in the backend right so if I hop over and set up my builder view to my opal view notice that it's been written for us right so another way that I could have arrived at the issue or that the answer is just writing my own opal but I don't have to because observe goes ahead and generates that for you as you play around with the UI. Cool so now that we have a sense around building opal queries let's take a back to our scenario so let's first of all quickly clear our query builder here go back to looking at just a raw data if you will one more point here that I'll put into place for us is that if you want to click this auto run button here it'll make sure that whenever you make a change the query builder it'll automatically run that query for you right just a little quality of life change there that you can make. In our scenario itself though we're looking at a recommendation service right so one of the things that we can do let's make sure that we filter into that service itself now if you don't see the recommendation service on the left hand side here it's probably because it's getting drowned out by all of the data over the last you know however much time so go back to the past four hours on your date selector at the top right of your screen and then we should be able to see. The recommendation service show up here so let's look for recco there it is right recommendation service right up here couple of different ways you could go about filtering this out you could use the filters on the left hand side right but you could also just go ahead and right container equals recommendation service on your query builder and it should take us there. So now that we've got this context into place there's further filtering that we can do right so for one we could make sure that we look specifically into our standard error stream now once again different ways that you could arrive at this you could use the filters on the left hand side you can click into the value in your table that you want to filter for we can see that actually our recommendation service exclusively sending logs out to standard error so really there's there's definitely something wrong there so now once we have this context in place we have the full sort of view into into our standard error. So the first thing. One thing that I do notice here though is that we have info logs coming out of standard error which doesn't seem to be optimal so for now what I want to do let's go ahead and filter into just the warning. specific word. So to do substring searches within observed, it's actually quite straightforward. You take the column name, right, the sketch log, and we use the tilde operator to do a substring search within the values of that column. So in this case, I want to do a substring search for warning. And as I do so, what it'll do is, first of all, show me only the logs that have the word warning in it, but also highlight that word for us within all my logs. Beautiful. So now we have it to be with just all of our warning messages. One thing is that this might be worth aggregating on for us to keep track of any changes that we made later on to try to fix this issue to see if the number of warnings are going down. Right. So what we can do here is make use of a dashboard for the first time by adding a little widget to a dashboard, a showcase us to us how many warning logs we have at any pointing time. So to do so, let's go back to the chart function. And let's look at it as a single step. Right. And instead of a count distinct, let's look at it as a count of values of all events. Right. So we have about 8.4, 3,000 warning logs over the last four hours. If you're a value differs, we're probably just looking at different timestamps. I see we have a couple of new attendees. So go ahead and the chat and create a trial count guys. If you want to follow with us, Kyle has put a link there for us. Just wanted to get that out of the way. But going back to our query here, we have that count right, eight and a half thousand warning logs coming from my containers. I'm going to add some information to this. So what we could do is we could choose a little label to add to this. I'm going to call it count of warnings. Right. And now we have that beautiful count there. Maybe we'll even change the color because eight and a half thousand is not great. I'm going to make it red to represent that it's not perfect. So just to show you guys right, all the different configurations that we can change for our queries, we can change to how it looks like. We can change the color by threshold by a value. It's very flexible in the way that we do so. From here, what we can do is take this query value that we have and add it to a dashboard. Right. Now we don't have a dashboard in place yet. So let's go ahead and create a new one. And it should automatically take that query into your dashboard. We can move that query around, drag and drop functionality. I just showcase it as dashboards a little bit around. And let's name our dashboard recode recommendation service. Why not? I just to keep track of that single point in time for us to keep track of our issue here. I'll give it a 10 seconds. So for you guys to catch up here, we'll keep going. One thing that I'll say to you, everyone, make sure we save the changes once we put these widgets in place, just because we're going to be adding more to this dashboard. And I don't want it to be sort of out of versioning with each other as we as we go along. So just make sure you save it. Just have it in place. It seems like I already have a dashboard called recommendation service rather from the other webinar. Let's call it recommendation service to beautiful. There we have it. Awesome, everyone. So now we have a place to lend evidence as we continue to investigate. Right. So let's go back to trying to figure out the issue here. What we'll do now is we'll actually use our logs explorer to pivot to what we call related metrics. Right. But first, let's make sure we are everyone zoomed out to less four hours. You're so have as much good data as we can. Right. So once again, to zoom out, go to the top right hand of your screen and just search for the past four hours. Right. I'm going to exit this visualization view that I have here. Go back to the raw data. And once we've pulled, you know, sort of the back back four hours of data, what we could do is use of the service correlation engine to go ahead and dive into this underlying container here by double clicking just the name. What that would do is it'll open up a little side drawer with all of the metadata behind that container, right, including image ID, restart count, because it's actually starting to find a bit here. The state of this container, the reason, right. And we also have a sort of correlated tab here, which takes us to the metrics of that container. So one of the things that I want to check for is to see if we have any memory, you know, any resource contention going on here, maybe my memory spiking, maybe my CPU spiking. So what we could do is just do a little string search here for any memory metrics. Right. And we see here, you know, container memory usage. It's all right. Not perfect, not great. What we could do is we can actually dive into our metrics explorer right from here. Right. So if we click on this, what would you call in your disk, where the error, or an arrow, it'll take us over to our metrics explorer and showcase the time series for us right from here, which we can they play around with and sort of change the representation to ads or dashboard later on. So one of the things that I want to do, perhaps, is look at my container's memory usage, throw it into my dashboard, and maybe even create an alert on top of it. Now, one thing that I'll mention as we go through this is that the the context of that container that I was looking at carried over, right. So now we're looking at the metric with my recommendation service container in there. And this is super vital when you're trying to response to an incident where you don't really clearly understand the root cause of the issue, right, being able to follow that trail of breadcrumbs as you go through a troubleshooting process and taking the context of one view to the next super powerful, why if you're an engineer trying to just figure out an issue out. As an aside, right, Opal was once again generated for us. So just tying backs what I said before, if you if you want to be a regular user observed, you don't want to dive too deep into Opal, you can just be a UI. And if you want to be that power user, you could totally know dive as deep as you want with an Opal itself. Now, we touched this upon sort of the initial architecture discussion, but I just wanted to reiterate a little bit for the folks who have lived this. It's always difficult, right, in terms of Opal or any proprietary query language, really, to adopt a new domain specific language, right. But the biggest difference within Opal, as I mentioned before as alluded to before, is that it works across all of your telemetry, whether it be metrics, logs, traces, business contextual data, CI, CD events. It's a single unified way to sort of query across the three pillars of availability, right, and more and much more. Many organizations nowadays are having to deal with, you know, using SPU for Splunk, from QF or Prometheus, maybe some elastic query language, right, maybe some SQL for the for the business types. In this case, no, you have one language Opal and it allows you to do everything sort of in one unified fashion. So now with that out of the way, right, the context out of the way, let's go to our second. Oh, Kyle, thank you. Let's go to our second quiz question here. So now that we're in this, right, go back to the builder, just make it easy for you guys. Now that we're in this view, the question is, once again, in the old tell demo name space, what three containers have the highest memory usage, right? So use everything that we've taught you so far to go ahead and find the three top containers with the highest memory usage. But once again, within just the old tell demo name space, so just as a hint, right, let's make sure we remove the context of the recommendation service and name space ourselves to old tell demo. So I'll give you guys a solid, like 20 seconds there, maybe 30 seconds to go through it. Right, we can give them maybe a solid 30. I think that's fair. Okay. And just in your case, to your best, you can see that this is a pretty wild environment as Luke is actually alluded to at the top of the call. So this is a live implementation of the open telemetry hotel demo. So this runs that same set of services that are distributed. And so the only thing that's always true every single time we run this hands on is that Kafka is like a totally memory, a total memory hog. But you know, it's a lot of work. That's okay. What I will say is that some things might jump around or ever. Do your best to answer the question. We'll still give you the certificate at the end of the call. So right on. Okay, we're going to get 20 more seconds folks. And so as Luke has saluted to earlier, I said earlier, sort of what are the three containers with the highest memory usage? Also, another fun fact using, so we have a basically we're connected to the head of the actual get repo for the demo. And so sometimes the stuff just changes, because we'll just do a pull and deploy, which is obviously like a cardinal sin in the world of DevOps. But you know, yellow, I guess. So cool. I'm going to have a poll and you walk through the answer. Awesome. Awesome. Show a couple of different ways once again, everyone. We probably could have arrived at this. I see some some great answers here in Kafka is in both them. So that adds up. First things first, let's make sure we names basically Oto demo, right? As we talked about before, run that query. So now we're just in Oto demo. And now we could. First of all, we could do this in a couple of different ways. Just for example, brute forcey by looking at it and try to find it. Notice that the values do change. So depending on your timeframe, you could have one answer. You could have had another, another timeframe. But only that we could have gotten at that is just look at a sort of breakdown of the sum by container, right? Over the last, let's see, four hours here. And that would have given us Kafka, add service, load generator, which doesn't come. It's just generating load. But then any one of those other services would have been an acceptable answer here. The whole point that we were trying to get to is just make sure that you're doing a sum by containers, right? And sort of aggregating it by what you're looking for to give you that answer. Well one thing that I'll sort of mention here, going back to our troubleshooting scenario is that it might be hard to see, but a recommendation service is not doing too hot, right? It's a little squirrelly compared to the other metrics. So one of the things that we can do is sort of just dive into it, right? We can click that recommendation service name. We can click on the time series from our recommendation service. It might not be as easy as it looks. So let me just filter us out recommendation service. There we go. I should make it a lot easier for us. There it is, right? It's looking a little squirrelier. You can have done this by clicking on the time series. It's just a lot of data there. So it becomes a little harder because it's filtered out a little bit more. But what I want to do with this now is do things. I think that it'll be worth adding this to our dashboard just so we can keep all of our sort of investigation steps in one place. So similar to what we did before with the query value for our number of warnings, we can add this to a dashboard just by clicking the top right hand side of your screen in actions and then add to dashboard recommendation service to or whatever you named your dashboard in that case. And if we go back to it and refresh it, there it is. Right? We can see that our new time series is right here. Beautiful in place for our investigation. Awesome. Now let's take advantage of what we have here already so far. And let's create a monitor off of it. Right? I mean, just going back to the troubleshooting scenario. The only reason we knew things were going wrong is because our customers were letting us know. So let's remedy that right now at Hawk and create a monitor. So that next time we know that this is going on without the customer being affected for a long time prior. Right? So one of the things that we could do is once again, instead of adding this to a dashboard, it's just creating a monitor for it. Right? And once again, observers going to take the context of the time series that you're looking for and then throw it into a monitor for you. So you don't have to refine that context. Right? Everything carries across your views. So this is the monitor creation patience to give you a little lay of the land. We have a time series of that metric over however many hours you're looking back. We have the query for that metric, which we saw before. So we're namespace Altademo recommendation service container. It's a sum of its metric averaged over 30 seconds. And then we can create our alert conditions. Right? So everything is a nice little, we have a nice little gooey here for everything. We can choose the status or I guess the severity of this monitor whether informational or important. We can change the type of thresholding that we do whether it be when the metric is equal to a threshold greater than a threshold less than a threshold. We can change how we evaluate when it goes over that threshold. And then how long for it to be above that value right when we alert you. One thing that I'll mention here is that while we're looking at a static threshold in this case, we can it's it's more than possible. It's expected that you can create dynamic monitors, right? Based on a specific metric just by using things like a moving average or a standard deviation, right? And we can create those sort of more dynamic thresholding monitors for you. For the case for the sake of simplicity, let's just keep this one static. And let's choose a decent threshold for us. So we can see here, you know, memory rarely rarely goes over this sort of 40 megabytes range. So let's put our threshold at around that value. The thing is that since we're going to be dealing with, I believe just bite zero, we're going to have to add quite a couple of zeros, but this should take us there. Right about 40,000 or 40 million. Yeah, 40 million bites should take us there. It's up at about 30 megabits. Seems like, but we see that threshold being generated there. And one of the things that I'll do here is change the type of thresholding just to show sort of a different representation of what the monitor would look like, right? So in this case, instead of having to stay above that threshold, I want to be alerted when my match across is at threshold, at least once. And one of the things that observe will do is highlight for you in that time series when the monitor was going to trigger, as well as show a preview of when this alert would have happened as well. We could also add different notification actions for this. Right, we can go ahead and do throw a webhook to your favorite communication collaboration tool, create a gerothaic or whatever you want here, webhooks, quite flexible. But once we have this in place, let's go ahead and save it. Oh, seems like I already have a threshold monitor in place, which is good. That's a pretty generic name. Let's go ahead and call our recommendation service, our rico service monitor. Hopefully that one doesn't exist yet. There we go. Right. So now we have a rico service monitor in place. Kyle, I saw you through on a quiz question here. That was my fault. Yeah, sorry. I forgot to close the last one because it moved the closed screen. No worries. No worries. Just wanted to make sure I wasn't missing anything there. Cool, everyone. So we've got our monitor in place. So if you want to take a look at it, let's just hop over to the monitor step. And then we should see there actually a kind of bit of monitors for this. But the latest one was just rico monitor, May 9th. And you should be able to see something similar. You can go ahead and believe click on it as well. And maybe not here. Anyways, there's our monitor, right? We can view the alert. There is. View the alert. And here we give you sort of all the the context behind that. But awesome. Cool. So let's take a step back here. We've we've fixed this problem or not having a monitor that is. We have a dashboard. We have our monitor in place. We'll be alerted next time. So now that we've investigated logs and metrics, I think it's only fair that we move on the traces to see what the customer experience might currently be feeling like for for our customers, right? And no better way to do that than to look at the requests that they're making to our website and how long they're having to wait to receive an answer to them. So if we pivot over to our traces, explore. It should be fairly similar to what we saw before. Make sure if you have no data here that you're looking at the demo data slash hotel data set. I was playing around with some open telemetry earlier today. And I was starting data into the sandbox. That's why I wasn't the other one. But you should be defaulted to this one. So I'm not too worried there. So once you've navigated to traces, let's expand our search to the last four hours once again, just to get some really good data to our platform here and within view. And by default, no, so there's we're filtering out your spans to service entry points, right? Meaning the first call to a specific endpoint, not necessarily the subsequent calls made from there. Just to facilitate, just to facilitate to me, if you're curious, you can go ahead and remove that filter. It's going to be a lot more data in your screen. But by default, we're sort of scoping to that. What we can do now, though, is quickly look at all of our error traces, right? They click the button filter out to only response status of errors. And then we see all of those calls. And there's actually quite a bit of them. A bit of errors going on here. We'll take a step back here, though. Take a little break for another question. Let me think of any context that I can give to help you guys before we dive into it. Now, this one, this one's easy enough. There's a couple of ways to get there and it's going to be a challenge for you guys. So the quiz is how many? Question three, right? How many? And what are the names of the services that we have error traces for? So notice that we're already sculpted into response status of errors. Notice that there's a couple of filters here for services. I'm wondering, is there one filter here that would help them, Kyle? Get there. Should we be sculpting into the namespace of demo data or hotel demo, you think? You know, we can do that. There's a few different sort of quick paths. I think what we'll do is let's give them 20 seconds. I think you give some very, very ample hints. I appreciate that. And then we can pivot from there. So we got here. Okay. All right, this. Nice. There might be a subtle trick to this one. So we'll see. Yes. So, yeah, that said, let's have you take it away. I'll end the poll. All right. Beautiful. Okay. There are a couple of different ways we've could have gone about this. Right? Everyone. So, for one, just by clicking on this little filter section here, we would be able to see all of the services that we have errors for, which is awesome. Right? It's already filtered out to that. A couple of other ways we could have gone about it is by looking at the filters on the left and side. Or even if you're feeling very adventurous, write some opal for it. Now, I'm not going to do that here, but it is one of the ways that we could have gone about it. Right? Just doing a little mate column to or create a table view or a time series or any whatever floats your boat there, if you're feeling adventurous. But the next question is sort of tie it into the same sort of context here, which service out of those four or five in this case have the highest number of client errors or traces in this case. I'll give you guys another 30 seconds or so 40 seconds to get that one out. There's a couple of ways you could go about this. You could brute force it. You could find something that I haven't shown you yet. Or just for your time series bar charts or whatever you want to do. Nice answer so far is certainly the least wrong. The least wrong for sure. I also feel bad. Most front-end teams, if you have, sorry, apologies to folks that are back. You work hard too, but most of the time, right? The way that your customers interact with whatever services you provide is through the UI. Usually that's where they propagate up, but it's not always front-end fault. Actually, it's very rare that's really the case, but for front-end folks, you're like, you come into this display and you're the biggest slice of pie on the pie chart. Feel a little bit bad. It seems pretty targeted, Kyle. Do you have some previous beef with front-end folk? No, no, no, no. As somebody who's incapable of writing any sort of quality front-end code, I think they get blamed on duty. All right, cool. So, answers are in. Let's take a look at it. Let's take it through a way to get there. Yeah, for sure. So folks, a couple of different ways. For one, you could probably just bet an eye on this and see that a lot of them are coming from front-end. On the other hand, we could try to use the filtering approach and observe. Always show you a little barks showing just how many of each value is found within the call. But the best way I would say, I guess the most correct way would be to group this little, we have some red metric bar charts here, requests, errors, duration. We could group that by a specific service, group the error times series by a specific service, and it will see that by far in large, the largest share is coming from my front-end service. So once again, just a couple of different ways we could have arrived at that issue, observe it gives you plenty, plenty of different ways to arrive at the same result. That being said, going back to our troubleshooting scenario, we can immediately tell that our customers are just not having a great time. Just given the sheer number of errors coming from front-end, that said, let's take a step back. And since we didn't see any errors being emitted from recommendation service, which is weird, it's what we were seen on our container logs. What else might be going on here? So let's do this. Let's scroll out to the attribute column. It should be here towards the end. So if you just scroll in our table view, all the way down to the attributes, one of the things that observe is quite powerful about, and honestly, one of my favorite features is the ability to manipulate your data within the platform. So we talked about the data-like architecture and being able to receive any sort of data, whether structured or unstructured. Here's a great display of how we use that power to allow you to sort of transform it post-injust. So scrolling over attributes here, if you click on the header of this column, we can see that it's a bunch of JSON objects. We can extract from JSON. So what observe we'll do for you is it'll look at that object, find all of the attributes within an object, and tell, hey, what do you want to bring into view here? And that's what we call schema on demand. So in this case, what I want to bring into view is my HTTP target attribute. So if I select that, and then I click apply, it'll do what it'll do is schema on demand will take that attribute, and on the fly, modify the schema of my table to introduce us that new column. That's quite helpful for a certain file, which endpoints are being hurt by this. Now, now that we're here, I'll give you guys about 10 seconds. I'll give you guys about 10 seconds here for us to catch up, but we have our fifth quiz question. So 10, 9, 8, cool. Here's the fifth quiz question. So once we have this attribute in place, the question is based on this HTTP target field, what four services are being impacted? Right? So using this newly related field that we just brought into view, which four services are being impacted? Here's about 30 seconds to get through it. Yeah, one of the favorite things that we did recently is this going to sound super nominal, because we talked about front end earlier. If you scroll left and right on a newly extracted field on a column, we'll actually read highlight it for you. So sometimes it gets a little bit dicey, China remember like, okay, I just added a new column or a new date or whatever. And so we can, we get a blue dot, of course, too, but there's a usually nice little sparkle effect we can be layered on top of that reason. So today I learned. That's right. All right, a variety of answers. I like this. All right, let's take them through it. They come through it. Yep. Cool. So everyone, once again, a couple of different ways we could have gone about this. I might be in the easiest one. We'd be sort of just about an eye on it. If you do a filtering approach, and we can see, right, we have a product endpoint, we have recommendations endpoint, I'm going to check out endpoint, and then all the way down, you have a data endpoint, right, which I'm not quite sure what that's doing, but it's there. It has errors attached to it. So it's probably investigated at some point, but that can go in the backlog. So if you said check out data product catalog and recommendation, you were correct. So good job there. So what we'll do here now. Let's filter into our, sorry, the question when the QSPA backup, let's just filter into our recommendation, sort of endpoint here, right, because you know, it's generating front and errors, or it should be by people making calls onto it. So let's apply this filtering here. Let's take a look at all of the HB targets that are just for API slash recommendations. Well, we can do here now, so that we're triggered, we're not triggered. We're scoped into my recommendations endpoint. Let's go all the way to the left, and it allows us to, oh, it actually, we don't have to go all the way to the left. I didn't notice this, but the view trace button is frozen in time so that we can dive into whatever row we find. We can dive into the underlying trace view for that, right? And you should, you know, you could be familiar with trace views, but it should give you guys a little end here. It's just going to be a flame graph of the HP call stack, right? So someone made a request to that first endpoint that we saw, which then called onto my recommendation service. And one of the things that that I'll do for you is highlight the spans that we have errors for, right, as well as bring into view, hopefully the stack trace, where do my stack trace go? Span events maybe? Yeah, it's also possible. There we are. Hey, yeah. So just a quick warning, guys, if Span event says zero, click on it anyways, because it'll update to show if there's any errors that I wish to probably don't think I look at this guy later on. But one of the things that the tracing will do, which is awesome, is it will highlight where you have errors, but also give you the metadata behind that error, right, which in this case, if we double click it, is the stack trace of that problem, right? So we can see that that sort of spiking memory that we saw in Mexico explorer, it's very likely triggering an out of memory, right, where we can't connect to my back end anymore, thus killing the service and sort of making it available for the front end, does why we have all those front end errors, right? So our front end is trying to call our back end. It's not able to and then it comes back with an error, which looks like in front in the error. So it looks like we can at this point, you know, pretty much fully confirm the user reported issues. All we need to do at this point is probably just page out the team that owns the recommendation service, come up with effects. And then in the meantime, we can sort of bump up the memory limits from my back end infrastructure so that it stops going out of range of rights failing. From what I remember, about 80 megabytes, you do the job there. The thing though is that if we hop back into our explorer, right, if we click on this little back button and get back to our traces, it seems that there's additional errors that we're seeing across a few different services here. And we can sort of continue through this trace explorer to get a better picture of what is going on, right? So close that flame graph view. Let's remove this recommendation service H to be target because we're already done with it, right? We've confirmed that that's something going on there. So let's get out of there. What I want you guys to do is to use one of these filters to look at a minimum duration of your traces, about 15 milliseconds, right? So on the left, on sort of the filter view, go ahead and do a minimum of 15 milliseconds first to look at those slow calls or another 15 milliseconds of slow, but this is for the sake of the story here. So once we're filtered into all of the traces that are above 15 milliseconds duration, we can see that there's still recommendation issues here, but it looks like our check out service is being uniquely impacted as well, right? And that's not great because that directly impacts our revenue as an e-commerce application, right? So let's dig into one of these traces right here. Anything that says status code 13 should be pretty good. Let's dig into one of these to see what that 13 is all about. So we can see that this trace is a little bit more complete than the others. We have more calls. We have more context, which is great. You guys can explore that if you want. I think it's an awesome little view here. But in this case, what to quickly reach into the root cause, we can simply just click on the first occurrence of an error. And I love this, right? We can just click on the first occurrence of an error within my flamethrough view and then observe is going to go ahead and highlight that for you of what that error is, right? Now, in this case, it just turns out that we had a feature flag to go ahead and fail, but just like a real deployment, right? It might seem like a pre-can process, but just like a real deployment, I mean, you start off seeing front and errors, you go into your traces, you find something weird, you hop into that trace, and then just by looking at the first occurrence of the error in your backend, you'll be able to get a pretty good idea of what's going on, whether it be an error message or the stack trace, right, at that point in time. Now, we can always disable this feature ourselves to remove one error path for our users, but that sort of concludes the first part, the first remote shooting part of our flow here. We're going to shift gears a little bit, and we're going to go towards sort of what we call our first challenge, right? And for that, we're going to hop back into our logs explorer view, still using our container logs demo data set, right? Container logs demo data, make sure we're using that source of data. What we'll do at this point is let's go ahead and filter into our controller application, all right, sorry, controller container, so with the click of a button, you can get there. And let's pretend for a moment that we never had a PM in place, right? Because the way we found their error in the last problem, the way we sort of got that root cause was by looking at our traces, you know, not all of the, not all of the teams out there are fully fleshed out with their delimitry in terms of a PM. So how could we arrive at that sort of same sort of context, just be our logs, right? In this case, ingress logs from our observer, in this case, engine X. We can actually get pretty far with just as information by taking advantage of what I talked about before, which was schema on demand, schema on read, right? On the log column header, if we look at this, our sort of web server logs, what we can do is we click on the column header, is do what we call an extract from strain, right? So unlike perhaps other observer solutions in the market, we don't have to create a pipeline for our logs. We can use the schema on read functionality to go ahead and modify once again, what we're looking at with the information that's within my logs, which once again doesn't matter how you send into us, we're sitting on a data lake, we can play around with them. So what we'll do in this case, we're going to write at redgex to go ahead and extract the information from here. So if we click on this generate redgex button, Kyle is going to go ahead and paste the redgex for you guys in the chat, just in case, you know, we can't get it done via the the oligipity assistant here, which I actually should talk about, right? Oligipity is sort of the baked in gen AI that we have with an observe where it will act as not only your AI assistant, but it'll also help you with sort of these non-miniall tasks, because it's certainly non-trivial, but it'll help you with generating redgex, generating opal, right, and just assisting where it can. So once again, Kyle posted in the redgex into the chat, just in case, for some reason, this button doesn't work, which is should. But once I click apply, what it'll do is it'll extract as much information from my log as it possibly can, right? Things like the address, the data, the call, the method of the request, right? The URL that we're hitting here. Anything of a number of new columns there, which we're highlighted in green, as Kyle pointed out later earlier on. Now, the two more interesting columns for me here, which sort of give me a sense of application monitoring are two. It's the URL that we're hitting, right? The endpoint that we're hitting here, and then the status code. So one of the things that I want to do is I want to extract some information from my URL by doing the same approach that we did before and performing extract from string. Once again, Kyle is going to go ahead and post the redgex for us into the chat. The biggest difference here is that, I wonder if that'll work. Let's click apply. Let's check it out. Yeah, so the redgex, the redgexing and redgex, which is kind of, you know, I just think I find that to be totally wild. The other thing I just noticed is it looks like somebody's using some sort of scanning bot, I think, based off of the URL. Oh, there we go. Perfect. So I would highly recommend using the backup, the backup redgex I pasted in the chat, but functionally, if you look at the URL that's in there, that's not sort of what I call traditional application traffic, which, you know, the great downfall of redgexing your redgex is when someone inserts. Absolutely. And the whole thing, you know, using LLMs is, it's incredible. If we were to just scope the data into a little bit more similar endpoints here, it'll work perfectly, right? It's just that we're looking at across a lot of different contexts here. You can see there's a couple of different types of URLs, or it does its best. And you could probably build off of what it gave you very easily. But in this case, or we have the pre-built redgex here, why not just use it? But just to get back to the story, we now have a new API endpoint, sort of column, so data. And then we also have a status code next to it. And if you put two and two together, that gives us a pretty good understanding of the performance of each one of my endpoints that I can then create a metric to alert me on top of. So essentially, what we're doing here is the arriving application-related metrics just via my logs, right? So what we can do is, let's go ahead and aggregate on these status codes, including the no ones, by simply clicking to start a visualize button, and it's a little not visualize, but chart button in this case. So what it's doing now is it's doing a kind of values, which is fine, kind of all events. But what I want to do is I want to break this down by URL first, so we can run that, just see what it looks like. Pretty messy. But then we can go ahead and add status code. Run that again. So what we're doing now, we're effectively, we're visualizing this as a bar chart, just sort of take us implicitly beautiful. We're effectively looking at the performance of each one of my endpoints as a metric. And one of the things that I'll highlight is that first, we have an instrument that our application, and second, I'm adding two pieces of cardinality to this metric, which if you're using most observable solutions in the market today, right? This will be considered 84 custom metrics. With it observed, this is just data, right? There's no extra charge for what we're doing here. This is just a metric that you're deriving, and it's just present within your logs. So we're extracting that and generating this for you, and no extra cost. Me as someone, you know, just as a side point here, someone who comes from Datadog, great product, but the custom metrics there are scary, scary scary complex. This is very simple. No baggage associated with it. Love it. Also, if you want to show them, so obviously, one of the things you did is you extracted status code as part of the initial rejects. If you swap URL out for, I think we call it API, I think, maybe it's API endpoint. Oh, API endpoint. Good call. And so slicing and dicing the data can be as sort of complex or as simple as you want it to be. And rejects is like a classic tool that everybody I'm sure on this call has had to use at one point or another. So we're doing the lords work trying to make that a little less onerous. I still, however, will continue to have bookmarked rejects 101.com for testing. I'm sure one day the AI will overlord, she'll figure it out, but today is not that day. So yeah, it'll get there at some point. And look, it's already made a lot easier than it used to be in say five years ago. 100%. 100%. Awesome. If we want, we could also just as a side here, we could change the time, the aggregate, the time that we were, the resolution is what I want to say, right? So that instead of aggregating this every 30 seconds, it might be more helpful to aggregate this, perhaps every five seconds or every minute. It's completely up to you. We just give deflexibility within the platform itself to look at your metrics as granular as you want. So just to recap, while having full spans and traces is optimal, we can actually take something as simple as an ingress log from your web server, just to see how our services are doing and then go ahead and alert on top of this. So we've already confirmed that there are some serious health issues within our recommendation service. But if we didn't have that data in APM, and we were in a pinch, observe can actually just take commonplace data sources like ingress logs and transform it into something that's pretty close to APM. Now that we have this view into place, we're going to go into our quiz question six. Number six, it should be which APIs are throwing a 308 status code. So once again, which APIs are throwing a 308 status code? Just as a hint, the answer is in front of us, but there is ways that we could make it a little bit more obvious just by playing around with the visualization. Absolutely. And because we took the time to break out our status code as a distinct filter robo value, you could even look at games, something like that. So we'll give you all 30 seconds or so to answer this question. Something else that you kind of you touched on, Lucas, and I think it's really important just to talk from our experience and working with a number of customers, especially in the last year is there is a cost piece as you mentioned. And so because we treat everything functionally as just sort of data that you can bend to your will. And we obviously have first cost support for the three pillars of observability. Logs are ubiquitous, right? And to go from logs to distributed tracing is a journey like customers were either at point zero or maybe they're 100% there. I don't see that super common, but it's really, really we believe important to say, hey, let's let's take the data you have and let's build something that everybody in your organization can benefit from. Whether or not you're you've done everything you've auto instrument everything via hotel or we spent that time. That's just that's hard. That's additional work on top of, you know, delivering actual new features to the services you already own and maintain. So this is why we do this this exercise is just to show everybody like, look, there's actually a way to to get a tremendous amount of value out of stuff that's just kind of sitting in the background as a sort of digital exhaust. So all right, let's end that poll and let's hey show them that shortcut. Let's see how it goes. Show them the shortcut. Oh, which shortcut. Ah, okay. So one of the things that we could do is by wait, one second. I'm trying to what was the shortcut now? I was, yeah, if you move, sorry, you're good. If you move status code into the filter bar. So where it says container post controller, if you need status code equals three, oh, yeah. That was missing something for a second. No, no, no, no, no, no, yeah, everyone. One simple thing that we could do right is that we know we're looking for a three like status code. So we could just do it search on top of it. And then it would just highlight all of the end points that have a three week status code. The other way we could have arrived at that, which is simply be by looking, you know, probably how a couple of us arrived at it, it's just by looking, we could buy an island and see where we're at. We could have used other visualization types like looking at a table list, right, a couple of different ways we could have righted it. Last single highlight is once again, all of our rejects, all of our visualizations, all of our filtering is transformed into all pulling the back in here. So for everyone who said all of the above, that was correct. Good job. So well, we're at the end of our troubleshooting process. So congratulations. We've sort of successfully navigated our scenario. And we discovered that two of our services were causing customer issues. And now we have a path to resolution. We do have one last exercise as I alluded to before. And this is very, you know, there's something that's very much unique to observe approach to observability, right. We're going to do what we call data shaping and turn some of our observability logs into business insights. And it's a lot of fun and product managers like Kyle love this functionality. Now, this will touch on a number of new concepts. So feel free to ask questions in the chat if you get stuck. If you need help understanding something better, no clarification, and so on. So let's hop into our bonus challenge of using worksheets. Now, understanding things at the service layer is essential to getting ahead of as well as fixing problems, right. However, anyone who's been in charge of sort of ensuring uptime for a service, whether it be monolithic, distributed, otherwise, knows that one of the hardest sections to answer during, let's say a post-institence review, a post-mortem, is the assessment-slash evaluation section, right. Particularly, we understanding if, you know, from what you have, probably had available to you during the incident, was that adequate to solving the issue? Now, rather than starting in our explorers, we're going to go to our data sets page, and we're actually going to, you know, our hands started with some pretty, pretty good opal. So if we hop over to data sets, we're going to be looking for our sort of base data set for the demo data namespace, right. So if we just, you know, once again, navigate to data sets on the left hand side of your screen, and then scope into demo data, and make sure we do demo data name. I forget the differentiation here, Kyle, I remember you explained that once, but I forget as to, simply the difference between these. Yeah, so, just make sure we're on the, yeah, go ahead. I was going to say, so the way that we sort of sort things and namespace them, there's a special namespace called no package, and that's the one we want. So when you see the package and observe, think of every data set that's not packaged, almost always going to be tied to that data stream that we just talked about in the architecture slides earlier, they give it this way. We're dumping observability events into the central location that's what we call a data stream, and then you can create data sets off of that, but every data stream has a sort of a root data set, or a base data set is what we actually call it. And so that's, that's why we say look for no package, so you don't get into a pre-shaped piece of data. That's all. Yeah. Makes sense. Makes sense. Awesome. So, so now that we're here, we've sculpted into it. Don't click on the data set itself. All the way to the right, inside of your screen, there's going to be an option to open this in a worksheet. And a worksheet is sort of like a playground, right? You're a sandbox when dealing with any sort of data sets within observe, whether it be a no package data set, a scout just explained, or in a very populated, already transformed data set. So once you open this in a worksheet, you should be taken to a view, sort of like this, where you have the once again, your tabular data, we have your query builder top, you know, top of your screen. But then we also have an opal section that we can write opal with and sort of our, our in page IDE, if you may, to write, writing some opal. Now, as Kaya mentioned, there's going to be quite a bit of opal that we wrote here. So he posted a link in the chat that takes it to sort of that pre-built opal. What I'm going to do with you guys is I'm going to go line by line on this opal. And I'm going to showcase how it's transforming the data. Just make sure I get the right information here. So once again, go ahead and click on the link in the chat. It should be a tiny URL with the opal prewritten there for you. Just copy and paste it into your opal sandbox here, your opal window. And then we're going to go line by line understanding what this is doing to your data and how it's shaping it. So again, if I run this, nothing should change. I have my sort of raw data there. What I'm going to do is I'm going to go back to 24 hours just to make sure I have as much data as possible. And then we're going to go line by line. So what the first line of opal is doing, it's creating a column named tag and it's filling it up with information that's within my extra column. So in my extra object, the tag attribute is being transformed into a tag column. So I'm going to go ahead and run that. So we have is a new column tag because it's all null. But that's okay. It's just because that there's so much data within here. And what I want to do is I need to go ahead and fetch that data from all of my, so there should be almost 70 million rows of data here. We can say that once I fetch from everything, then that product catalog shows up. So there we have it. So then what we'll do is we're going to go ahead and filter specifically into that product catalog. So filter tag equals product catalog. We can see that these rows are coming out into view. I as observed is sort of parsing those 70 million rows really fast in about seven seconds to it. Well, then do is create a new column called product products, which is based off of a fields object. I should say a fields column within a products object. And then we have it right. Our new product column. It's here. One thing that I noticed is that it's currently an array, right, which honestly goes back to what we talked about before. It doesn't matter how you send the data to us. We can shape it after the fact where so we even have an array of JSON within observed and that's honestly pretty cool in my opinion. What we need to do though is flatten that array. We can then access the objects within it and transform each one of those into a column itself or a row itself, I should say. So now we have all that into view. Finally, we're going to take all of that information that we just went through. Then we're going to parse it into different columns so that we can access them a little bit easier. Right. So within all of those JSONs within that array, what my opal is doing here is it's accessing them since we just flattened it and then creating new columns based off of the attributes within those JSON. Then we're going to do some operation, some operating on one specific column, which is price, where we're going to have to add two values units and nanos and it's just being sent in this case. We're going to make them into a string, concatenate them. That's a social part of opal here concatenate them, turn them back into a float and then round them to the nearest set. So what we should be able to see here is the price of each product that we just arrived from two strings. Finally, we really wanted to magic happens in this case is we're going to take all of this data and we're going to transform it into a resource dataset so we can track the state of my products across time, things like changes in price, changes in description. I were basically going to be creating a product catalog in this sort of format here. We have the product name, we have the price of the product, the currency's in, the categories associated to in the description of it. Beautiful. Finally, we're going to set a primary key or I should say a label to this product dataset, which is based off of the name. All of this does everyone is that when I reference this dataset within a different one, this is what it's going to bring into view. It's going to be the name of the product. So it's creating a label is based off the name column. So with all that opal, right, just to recap what we did, we went from the raw data, which was this into an awesome resource dataset that showcases all of my products and it's tracking the state of them across time. I just want to say again as product manager, really loving this. Awesome. I mean, you came up with it, right? So I hope so. So everyone, once this is opal successfully runs once we have something like this, go ahead and rename this query, right? Some of the pop life inside of your screen, you can see as we do new queries, it's going to create new worksheets or new windows into your worksheets. Let's rename it. Kind of the top. I want to rename it. What was it called? Astro products. It's probably going to give me an error because I definitely have an astro products already. So I'm going to call it astro products five. Beautiful. If we want, we can go ahead and save that worksheet. I recommend doing so. I would save it. But then what we can also do, it's published this dataset, right? As a new data set. So what this will do is just make a permanent. So other people can access and reference to it and relate back to it. So once I publish it, I'll rename it to astro products again. So we can identify it. See, I'm going to call astro products five. Just make sure it's there. Click publish. What that'll do is make that dataset that view that we just created permanent so we can reference it back. So with our new data set created, let's go back to our dataset view. Let's search for astro products five. Notice that there's one, two, and five now. And there we have it, right? It's currently initializing. Observe is sort of spinning up its gears and swarming up the engine to create this resource dataset for us. For the next step, though, we're not going to stop there. Let's go ahead and look for container logs, sculpt into demo data. So search for container logs. Make sure we're using the demo data package one, not the Kubernetes one. We've made some changes. We can see this one actually has data in it too. And let's similarly open it into a worksheet view, just like we did before. Now, what we're going to do is we're going to predominantly use the UI to build the Opel here, just to make sure that we're used to that too. And just make things a little bit quicker. First things first, let's go ahead and filter into my cart service container. So go ahead and set the container header and make sure we're looking at cart service, click apply. It's going to bring in the filter into view for us here. Now, once we're here, we want to extract some information. I guess you guys can kind of see where we're going with this. We created a product catalog and now we're looking at our carts. Let's hop into the log message and let's do an extract from string. But instead of using rejects this time, we're going to do one of the prebuilt parts that we have for key valley pairs. So we're going to try to extract some key valley pairs from this. So once again, column header view, extract from string, key valley pairs. Click apply, observe, do its best to go ahead and try to extract any key valley pairs that identify, identify from your log messages. Build it into that new sort of KV pairs column. Now, we're going to do one more extraction here, but this time we're going to take advantage of the JSON structure of our new KV pairs column. So once again, go into the header, do an extract from JSON and let's extract all three fields from that column and bring them into view for us here. All right, so there we have it. Product ID, quantity of products, and another user ID as well, which is really nice because we can now tie back in how much each customer is adding to their cart in terms of product value, right, because we have that information now. Just to highlight that, again, the opal is built for us via the UI actions. That's sort of super helpful. And now that we have our three new columns here, what we're going to do is we're going to use the power of graph link. The correlation that observe creates across everything to correlate this data set to our astro products data set that we had before. So on the ellipses here on the top left inside of your screen, go ahead and click on link to other data set. What that'll do is it open up this nice little GUI first to choose which data set we want to link this to and I'm going to link it to astro products five as we just created. And we're going to link this via the column of product ID, right, because notice that we have a product ID interview and we have a product ID in my my astro products data set. So we're going to link into this and then the link name is just going to be astro products. I just first know what's going on there. As I click apply, remember how we set up the name of the product as that initial label. So that's what we're bringing to view. We bring in a new correlated column with the name of the product, right, and it's in green, meaning that there's a correlation to back in there. So what we've done is we've just joined our new our first data set and noticed that once again, the default name is coming in as the label. But what I could do is I can add related fields from that old data set that we just had, things like the price, right, things like the name, the categories description. I can bring everything into view here and we can sort of apply and once again use schema on the man to modify this schema of my once again, logs you as what we started with. Now one more thing that I'll sort of point out here is that observe types all of its data. So if I want to take something like, let's say price and it's a string, I need to make sure that I modify it or change converted to an int, right, before I can go ahead and build a visualization topic. And then once again, sort of writes the opal for us everything in the back end. So worksheets have us like different flow to creating visualizations than that explorers do. But still, we can click this chart button here and it'll give us sort of a count of how many products are added or how many products are logs we've seen. What we want to do is just do a quick sum or is it some of astroproducts price by astroproducts. And hopefully what that will give us is how much we're getting paid by each product, right. And someone like Kyle would love this because they have some business insights as to where the focus or efforts on it seems like optical tube assembly is super, super proper. Big fan, yeah, optics of any kind. Yes. I noticed I might have used the astroproducts price when I should have used the currency or the valley. I don't know. In any case, it seems like we're making a lot of money off of this and it's the same end goal. That's right. Now, you know, just to sort of save this for later use, we can just add this to the dashboard to be greater earlier, right, like the nation service to here. Bring that into view. And if I refresh this, we should be able to see that third beautiful bar chart that we just created to keep track of, you know, how much money we're making off of each product. Awesome. Perfect. Bonus question. And I guess our last question here is going back to this worksheet right here. Which product by name do we have a question for this one, Kyle, Crebel? Or is this just a bonus one for them to figure out? Yeah, it's about it. So they don't have to answer your V as you them. They just want them to think about it. Just really feel it out. A little challenge. Yeah, a challenge. All right. So the question, third question is everyone. Which product by name has the highest number of ads to cart actions in the last four hours? So not price actions. How many of them have been, what's the product has been added the most? Think of it as your PM giving you some homework to figure out more business insights. Awesome. I think we're done here, right? We've covered, let's just recap what we covered. We went through a troubleshooting workflow to figure out what was wrong with our recommendation service within our front end and in our e-commerce application. And then as a little bonus, we found out that the other problems within some other services that we were able to remedy there. Then lastly, we went through some opal exercises. We built some really nice visualizations here and added them to a dashboard. And we hopefully learned a lot along the way of observing the powerful plan from the beef created here. So just to sort of finish some housekeeping items here, everyone. First of all, thank you. I thank you for joining us through today's workshop. You're going to get a survey probably from Grant to give us some feedback on this. So it'd be awesome if we could get some insights there. The trial that we just created is going to be yours for you to use for the next 14 days. So if you're for to play around with it, add some data, your own data if you want, just explore as much as you want. If you're interested in a data shaping session, reach out to schedule some time with Grant. And please feel free to join us for our next webinar with our one and only keep buzz. Well, one of our other sales engineers here within Observe is going to be a workshop on navigating regulatory compliance with observability in modern approach on May 16th. Love it. You enjoyed this one. You loved that. But that'll pass it back to you guys, Kyle and Grant. Yeah. So just for me, thanks everybody for signing up. We were actually using Observe to watch folks sign up because we've actually instrumented the provisioning code. So it's always exciting to see folks log in. So thank you for taking time today. I know time is precious, right? So really sincerely appreciate folks just going through this and Lucas obviously for doing such a great job. And I will do a shout out as a product manager of a SIM product years ago, regulatory compliance near and near to my heart, probably the nerdiest thing I've ever said. But it's actually pretty good. So if you have colleagues on the cybersecurity side of your teams, encourage them to check this out. We actually have some pretty interesting stuff to offer on that front. So I then Grant, if you want to close this out, my man. Yeah, thank you so much, Lucas and Kyle. I'll have everyone's observability innovator certificates out no later than tomorrow. And I hope to see everybody on our upcoming webinar. Thank you so much for running this Lucas and Kyle and I appreciate everybody's time today. Thanks everybody. Take care. Have a great one. Take care.