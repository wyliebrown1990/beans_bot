 Welcome everyone. Welcome to the Observe hands-on workshop. My name is Lucas. I am one of the sales engineers here on the Observe team. Today, I'll be guiding you through the troubleshooting workflows slash Observe 101 and how to use the platform I'm joined by Kaya Champlin, one of our product managers here as well, and he'll be facilitating some of the content through the chat. If you have any questions as we go through this, feel free to throw them in the chat. Kaya will be there to help us out, send some links, documentation, and so on. So let's get started. Cool. Now, before we actually hop into it, a little inspirational quote from our host, Ron Grants-Wonsen here, the secret of success is to do the common thing, uncommonly well by John D. Rockefeller. So here at Observe, we do have this philosophy that everyone's trying to do observability and some people are doing it. All right, some people are trying their best. But we do think that we found sort of a really magic formula here in terms of how we're tackling the problems in this industry today. So as we go through this workshop, you know, keep this phrase in mind just to get us bumped up for the session that's coming up. One cool thing that will provide you after you complete this workshop is a little certificate signed by our CEO, Jeremy Barton. Quick little fun fact about Jeremy is that he's on the board for the McLaren Formula One team. So for a big F1 fan, you know, you can go there and show your co-workers his signature, whatever floats your boat there. But yeah, in terms of the agenda, quick breakdown here. The first 10 minutes, I'm going to have you guys create a trial account with an Observe so we can really can navigate through this process together. So if you can, if you could throw this link in the chat for people to click on it, go to htpsaccount.observe.com. So you can create your account as I go through the rest of the agenda. So go through this little presentation here. We're going to do a quick overview. On the actual workshop, we're going to talk about the architecture behind observe and some of the benefits that come along with it. And how we're differentiated in the market from other ability solutions. I'm going to introduce you guys some terminology behind observe, namely data streams, data sets, hopefully you're going to be hearing these words a lot. They really tie in everything together of what we're doing here in our texture. And then we're going to go through an overview of the actual environment, right, the hands on over environment, which is really your free trial. All the data that we're seeing here is going to be based off of the open fly, which is trying to be shop application. Right, so we're getting a lot of good metrics from from there. And we're going to do a troubleshooting process on top of that, right. Now for the remainder of the time, we're going to be doing the hands-on exercise. So the actual exercise is going to be a troubleshooting, containerized service. I'm going to have some nice quiz questions for you guys as we go along. To make sure that we're really putting the knowledge that we're gaining into use. We're going to do some data shaping 101 towards the end, which is going to be a bit of a challenge where we're going to use Opal, right, our processing language to go ahead and transform some data in some meaningful ways that we can use later on. And towards the end, we'll have plenty of time for Q&A. Anything you guys have any questions, anything that's not quite clear, anything you want to learn more about will be here for you to answer those questions. So with that in mind, let's talk about our texture, right. And I have just a quick sort of, I think it's important to use this diagram here just to illustrate how observe is different than in other solutions in the market, right. And the way I like to describe observe everyone is that we were the first observability solution that was built upon a data lake, right. The data lake in this case being snowflake. And we'd like to split sort of the understanding of this into two parts, data in-just part and data transformation part. So if you follow with me here on the left hand side of my screen, notice that observe ingests to let me show you from all your major cloud providers, all your major tools, whether it be host-based containerized Kubernetes, AWS, GCP, Azure. One of the things that we pride ourselves in is that we rely pretty heavily on open source technologies, right. We our team saw where the industry was going in terms of ingest and we realized that open source and the avoidance of vendor lock in is really the future of observability. So we use things like Prameepis, Grafana, Open-Toliner-Tree to facilitate that. Because of our data later texture and because of the separation of storage from compute, we're able to provide you 13 months retention and all of your data by default. And all of your data, really that comes into data lake, there's those trick ingest schemas on that, right. So if you guys want to send us structured unstructured JSON, plaintext, there's really no sort of convention issue how you want to send the data to us because all of the transformation is then post ingest. And the last thing I'll mention here in terms of the data that's coming in, we're not necessarily married to just observability data as well, right. As a matter of fact, you're observable, believe that observability is more of a data problem than just a telemetry problem. So we expect you to be sending us things like CRM information, right. You're ticketing information. You want to send us sort of jira's end-ass information you can, business contextual data, right. That we can then correlate to your telemetry and give you a full visibility into your actual application. So now that we've talked about data ingest, then we do our data transformation, right. And we'll see a lot of this throughout the following slides here. But just as a quick high level, this is what we call our data graph, right. Our data graph is split into what we call data sets. And those all are originated from what we call data streams. We're going to go into more detail on this, but I just want you guys to keep in mind that your data comes in into what we call our amount of machine data. So let's say amount of Kubernetes data, amount of AWS data. And then we shape that data into the meaningful views that are useful for your team. So for example, in the case of Kubernetes, we take all of your Kubernetes data and then we shape it into a container data set, a pod data set, right. A logs data set for all of the information that's coming from Kubernetes that's more meaningful to you. And we do this automatically. But let's dive a little bit deeper into those that terminology, right. That we that I've been mentioning here. So let's start off with data streams, right. Which is really the beginning point of where your data is coming from. So as you're seeing here on this slide, observe collects all data, system and application logs metrics and traces into observations, which are associated to specific data streams. You can imagine it's a source of data. Data streams are a flexible way to manage data ingestion. And each data stream sends raw observations to a source data set, right. Meaning that for each source of data that you have, we're going to have a raw data set that has all your data in it, as we'll see here in just a second. But then we've been talking about data sets, right. And data sets are derived from data streams. And they're structured with times or time intervals, as well as relations linking to or from other data sets. All right. So as we can see here on the right hand side of my screen, because you see that all of the data sets are interconnected because they're derived from one another going all the way back to that data stream or that raw data set that we're talking about. Now a data set has a schema, which is a set of named columns and definitions of data types and stored in those columns. All right. So as we take a look into data sets, you'll see that they look very familiar to what a SQL table would look like. Right. And so it should be something that you're used to seeing. But we offer a lot of flexibility on top of that. The last thing I'll mention here is that notice that there's two different colors for these data sets. Purple data sets is what we call an event data set. Right. And those events have timestamps or time intervals attached to them as I just discussed. On the other hand, blue data sets, they're tracking the state of an object through time. Right. And that object could be anything from a pod to a user to a customer to a node to a Damon set. Right. Things that are constant through time who state can change. The last thing I'll talk about is Opal. I've been sort of alluding to Opal, which is our observed frosting and analysis language. Now Opal is a query language for searching and transforming data within observe. It is a streaming language that allows you to both shape and query your data using the same underlying functions and verbs. It is purpose built to handle high volume times here's data and works across logs metrics traces and anything else you send us. Whether that be telemetry or business contextual data as I talked about before. Now one thing I'll mention here is that you might thinking, Oh, Lucas, you know, another query language. The beauty behind Opal is that everything that you do in the UI as you'll see. It's transformed into Opal in the backend. So if you want to be a regular user of observe who doesn't really dabble in Opal all that much, you can. Well, at the same time, if you want to have someone be a power user of observe, and they can really get deep into Opal and learn all of the sort of on teachers that that sit behind it. Right. So I just leave that with you guys. You'll see as we go along, everything with you in the UI is transforming to Opal in the backend. So on the environment itself, right. If you guys haven't yet created your your free trials, please go ahead to that link that kind of put in the chat here for you to create a trial so we can so you can follow along with us. And once again, everything's based off of the open telemetry demo. One thing that I'll say is that all the data that they will be dealing with is going to be namespace to demo data. Just to make it easier to follow and and locating things that being said, if you guys want to explore the platform and go off the beaten path. I feel free to do it. We're not we're not tied to just the data that we're looking at here. We want you to explore and learn as much as you can. Now, if you want to interact, right, I'll sent the the link to the application there. So you guys can go to the URL and an interactive website itself. Once again, all of the data is based off of the open telemetry data. And we'll be collecting logs, metrics and traces from it via our Kubernetes and old tell integrations. Right. Double click on it once again, everything would be namespace to demo data. But feel free to go off the beaten path if you want. On the workshop itself, just get a little bit deeper on the agenda is we're going to be walking through how to get familiar with the explorers within observe, which is what we call our unopenedated views. We're going to be using logs, metrics and traces to triage a user reported issue in this case. And then from there, we're going to build a dashboard sort of keep our investigation in one place and put all the context that we learned there into it. Right. We're going to be creating a monitor as well based on raw memory metrics so that we can sort of pinpoint this issue a little bit better later on. And then we're going to quickly build a new data set at the end to enrich our logs. And that's where we're going to learn about worksheets. And once again, add that back to our dashboard. Awesome. So with all of that information, all those housekeeping items out of the way. Let's. Let's jump in so just move my zoom window around here a little bit just make it easier on me. Hopefully everyone has their own. There's one more light slide here for us. Hopefully everyone has their trials up by now. That's the thank you slide spoiler alert. Let's hop into the actual environment. Hopefully everyone has a trial account by now. If you don't, if you have any questions about that, please. Through for the throat in the chat. I'm called will be more than glad to help you guys out. Awesome. So folks, everyone, welcome to observe. This will be the the welcome screen, but. I don't want to spend too much time on here. I actually want to get right into it. And the first place I want to introduce you guys to is the data set graph right will have been alluding to of how everything is interconnected with an observed. So as you hop into your trial, make sure we go into the data set graph piece here. And click on this little lineage view for us to get a good understanding right of what observed looks like from a 50 foot thousand view. So this is the graph for everything is interconnected. You should all the data sets are derived off of each other. As I mentioned before, we're going to be working off of the demo data name space. So please go ahead and make sure we filter out to just demo data so we can see all that a little bit closer here. Right. So right off the bat, you can see that we provide a number of ready to use apps that sort of model common infrastructure and application environments right things like AWS as you see down here things as. Kubernetes as you see in our data sort of namespace here things like open telemetry right for tracing and spans and application metrics. And as I mentioned in the terminology discussion demo data is the data stream, but it also exposes what we call our source data set right so what we've gone ahead here and done is use the Kubernetes and hotel integration. So then derive these materialized views of important context behind hotel and Kubernetes right things like metrics log traces pod health container health and so on. Now we'll get into the data graph a little bit more later on for now let's talk about today scenario right your mission should you choose to accept it in this case. So for today what we'll be doing is we're going to be putting on the hat of an SRE for a digital business, which is our astronomy shop and ecommerce website. And our team is responsible for ensuring the availability of the shop and responding both to regular on call cases, as well as any pages if your customer success team know gets reports that the application is going down or any potential issues. Today just happens to be one of those days you've been receiving a lot of problem reports from customers and they're indicating that the new important you may also like feature might be broken and when I say you may also like I mean like a recommendation feature right you bought a table you might also like a chair. No alerts have been going off yet, which is something that we must fix right because if there's issues going on we should probably have alerts on top of that. But we're going to do some initial triage with an observe anyways right so let's go ahead and get familiar with one of the first observed explorers that we're going to touch upon which is our logs explorer. So on the top left of your screen go ahead and click on to our logs explorer here. So we can gain view into all the logs that we have within our platform right now. So just to give you guys a quick sort of way of the land on what we're looking at here the way observe works and it looks like it's pretty similar to other solutions right if you've had to grab your way through issues. This sort of you should be pretty familiar to you. So on the top left inside of your logs explorer is where you choose the source of the data that you're looking at right in this case make sure we're looking at container logs coming from the demo data namespace so we're all looking into the same context here we can figure out what's what's wrong with our with our application. Now on the bottom of that we see filters right which really just map to the name of our columns right are sort of a tabular view as I mentioned before so we can go in filter into the values of those columns. So for example if I want to look into the logs coming from let's say the scarred service container I can click on that and you see that the context box plates right there right and then a couple of seconds here we have all that context in mind. So just to showcase that. And then we have ways of filtering into every single column as you see there. Now as you selected filters you can see that it's sort of update the updated that so this dynamic green builder and let's make sure that we're looking into our hotel or demo data namespace right our hotel demo namespace just make sure we're looking on the same data. And a couple of things that I want to showcase you guys here is that when we're filtering for specific information within our logs. I was going to do its best to sort of provide you a common values to search for right so in this case as I went into the space you sort of recommended me looking to all tell them because there's a lot of logs within that name space right and it just helps you out of complete. It also gives you sort of a summary of the possible operations that you can do with your logs here including things like substring search and so on. The one last thing that I'll mention for us here is that we're not sort of tied to this that we'll review either we can dive into what we call this chart view and that's important because it's going to take us to our first question of the day. The first question and the reason I've taken you guys to this sort of chart view is as a little hint is how many containers in the hotel demo namespace are sending logs to astroshop in the last 60 minutes. I'll give you guys one more hint we can choose a type of function that we're looking at right from here. And we can also choose the type of visualization that we're looking at right from here right so instead of a line chart that look at it as table single stat and so on. So let you guys I give you guys 30 seconds or so to go ahead and. Get that out maybe maybe 45 seconds maybe a minute Kyle are you able to throw the question out to them the question oh that is my bad I totally was typing to you not. Alright so the first quiz question as you go through it and we'll give you about a minute to answer this is as Lucas said sort of how many containers are in the hotel demo namespace. And just another bonus hit yeah show you can everything in observe is time series and so this idea of sort of defaulting to sort of a timeline views what we do we were pretty proud of that but. Summary is also something you may want to attempt to do. Just as it heads up so we're going to give you 30 more seconds but the question and answer is a polar quiz in zoom so you just go in when it pops up and click the answer to it as you see fit. And if you don't feel comfortable answering that's okay to it's okay to. So okay to. Looking strong nice i'm not going to say that is correct but it might be the most correct. It's definitely the least wrong it's the least wrong it's a good way of. There's actually some inside inside baseball there because things are time series the sort of rising and falling edge of things from sure folks have dealt this before you can sometimes go a little bit above a little bit below that's why we say if you do a summary that will be the sort of actual concrete. Summary every time. All right i'm going to end it go ahead and show them how it's done look yes. Perfect appreciate it Kyle so everyone there's multiple ways we could have gone about this and i'm sure you found your your respective ways you guys did get the correct answer 15 containers. How would you arrive at that right a couple of different ways as I said so for one already namespace sold out demo that's great. We can change the visualization type to be a single staff right which by default what I'll do is give us a single career value but in this case we're looking at a count of all events right what I really want. One is a count of distinct containers. Right and that should take us to that 15 number that we saw before now once again we could have used this you know we could have done this as a table we could have done this as a flying chart but the answer is 15 so good job everyone who got it right. Perfect how do I close this this quiz question now there we go it's gone. Awesome awesome so as some bonus points here we can also take a look at the you know as I mentioned before I just to bring that into view. Anything you do in the UI within observe is starting to opal in the backend right so if I hop over and set up my builder view to my opal view notice that it's been written for us right so another way that I could have arrived at the issue or that the answer is just writing my own opal but I don't have to because observe goes ahead and generates that for you as you play around with the UI. Cool so now that we have a sense around building opal queries let's take a back to our scenario so let's first of all quickly clear our query builder here go back to looking at just a raw data if you will one more point here that I'll put into place for us is that if you want to click this auto run button here it'll make sure that whenever you make a change the query builder it'll automatically run that query for you right just a little quality of life change there that you can make. In our scenario itself though we're looking at a recommendation service right so one of the things that we can do let's make sure that we filter into that service itself now if you don't see the recommendation service on the left hand side here it's probably because it's getting drowned out by all of the data over the last you know however much time so go back to the past four hours on your date selector at the top right of your screen and then we should be able to see. The recommendation service show up here so let's look for recco there it is right recommendation service right up here couple of different ways you could go about filtering this out you could use the filters on the left hand side right but you could also just go ahead and right container equals recommendation service on your query builder and it should take us there. So now that we've got this context into place there's further filtering that we can do right so for one we could make sure that we look specifically into our standard error stream now once again different ways that you could arrive at this you could use the filters on the left hand side you can click into the value in your table that you want to filter for we can see that actually our recommendation service exclusively sending logs out to standard error so really there's there's definitely something wrong there so now once we have this context in place we have the full sort of view into into our standard error. So in the first extreme. One thing that I do notice here though is that we have info logs coming out of standard error which doesn't seem to be optimal so for now what I want to do let's go ahead and filter into just the warning logs into a substring search in our logs for that specific word right so to do substring searches within observe it's actually quite strict forward you take the column name right the scape log and we used the till the operator to do a substring search within the values of that column right so in this case I want to do a substring search for things that we could use in the past and the best case is the word is there. I want to do a substring search for warning. And as I do so, what it'll do is, first of all, show we only the logs that have the word warning in it, but also highlight that word for us within all my logs. Beautiful. So now we have it to be with just all of our warning messages. One thing is that this might be worth aggregating on for us to keep track of any changes that we made later on to try to fix this issue to see if the number of warnings are going down. So what we can do here is make use of a dashboard for the first time by adding a little widget to a dashboard that showcases to us how many warning logs we have at any pointing time. So to do so, let's go back to the chart function. And let's look at it as a single step. And instead of a count distinct, let's look at it as a count of values of all events. So we have about 8.43,000 warning logs over the last four hours. If you're a value differs, we're probably just looking at different timestamps. I see we have a couple of new attendees. So go ahead in the chat and create a trial count. Guys, if you want to follow with us, Kyle has put a link there for us. So I wanted to get that out of the way. But going back to our query here, we have that count right 8.500 warning logs coming from my containers. I'm going to add some information to this. So what we could do is we could choose a little label to add to this. I'm going to call it count of warnings. Right, and now we have that beautiful count there. Maybe I'll even change the color because 8.500 is not great. I'm going to make it reds represent that it's not perfect. So just to show you guys right all the different configurations that we can change for our queries, we can change the how it looks like, we can change the color by threshold, by a value. It's very flexible in the way that we do so. From here, what we can do is take this query value that we have and add it to a dashboard. Right, now we don't have a dashboard in place yet. So let's go ahead and create a new one. And it should automatically take that query into your dashboard. We can move that query around, drag and drop functionality. I just showcase you guys dashboards a little bit around. And let's name our dashboard, Rico recommendation service, why not? I just to keep track of that single point in time for us to keep track of our issue here. I'll give it a 10 seconds. I'll show you guys the catch up here. We'll keep going. One thing that I'll say to you everyone, make sure we save the changes once we put these widgets in place just because we're going to be adding more to this dashboard. And I don't want it to be sort of out of versioning with each other as we go along. So just make sure you save it, just have it in place. Oh, it seems like I already have a dashboard called Recommendation Service, probably from the other webinar. Let's follow it. Recommendation Service 2. Beautiful, there we have it. Awesome, everyone. So now we have a place to lend evidence as we continue to investigate, right? So let's go back to trying to figure out the issue here. What we'll do now is we'll actually use our logs explorer to pivot to what we call related metrics, right? But first, let's make sure we are everyone zoomed out to less four hours. You're still going to have as much good data as we can, right? So once again, to zoom out, go to the top right hand of your screen and just search for the past four hours, right? I'm going to exit this visualization view that I have here. Go back to the raw data. And once we've pulled, you know, sort of the back four hours of data, what we could do is use Observe's correlation engine to go ahead and dive into this underlying container here by double clicking just the name. What that'll do is it'll open up a little side drawer with all of the metadata behind that container, right? Including image, ID, restart count, because it's actually starting to find a bit here. The state of this container, the reason, right? And we also have a sort of correlated tab here, which takes us to the metrics of that container. So one of the things that I want to check for is to see if we have any memory, you know, any resource contention going on here, maybe my memory is spiking, we might see it to use spiking. So what we could do is just do a little string search here for any memory metrics, right? Now we see here, you know, container memory usage. It's all right, not perfect, not great. What we could do is we can actually dive into our metrics explorer right from here, right? So if we click on this, what would you call in here, the square of an error, or an error, it'll take us over to our metrics explorer and showcase that time series for us right from here, which we can they play around with and then sort of change the representation to apps or dashboard later on. So one of the things that I want to do, perhaps, is look at my container's memory usage, throw it into my dashboard and maybe even create an alert on top of it. Now one thing that I'll mention as we go through this is that the context of that container that I was looking at carried over, right? So now we're looking at the metric with my recommendation service container in there. And this is super vital when you're trying to respond to an incident where you don't really clearly understand the root cause of an issue, right? Being able to follow that trail of breadcrumbs as you go through a troubleshooting process and taking the context of one of you to the next, super powerful, right? If you're an engineer trying to just figure out an issue out. As an aside, right? Opal is once again generated for us. So just tying back to what I said before, if you want to be a regular user observed, you don't want to dive too deep into Opal, you can just be a UI and if you want to be that power user, you can totally dive as deep as you want with an Opal itself. Now, we touched this upon sort of the initial architecture discussion, but I just wanted to reiterate a little bit for the folks who have lived this. It's always difficult, right? In terms of Opal or any proprietary query language, really, to adopt a new domain specific language, right? But the biggest difference with an Opal, as I mentioned before, as I alluded to before, is that it works across all of your telemetry, whether it be metrics, logs, traces, business contextual data, CISD events, it's a single unified way to sort of query across the three pillars of observability, right? And more and much more. Many organizations nowadays are having to deal with, you know, using SPU for Splunk, from QF for Prometheus, maybe some elastic query language, right? Maybe some SQL for the business types. In this case, no, you have one language Opal and it allows you to do everything sort of in one unified fashion. So now with that out of the way, right? The context out of the way, let's go to our second. Oh, Kyle, thank you. Let's go to our second quiz question here. So now that we're in this, right, go back to the builder, just make it easy for you guys. Now that we're in this view, the question is, once again, in the Oltel demo namespace, what three containers have the highest memory usage, right? So use everything we've taught you so far to go ahead and find the three top containers with the highest memory usage. But once again, within just the Oltel demo namespace. So just as a hint, right, let's make sure we remove the context of the recommendation service, and namespace ourselves to Oltel demo. So I'll give you guys a solid, the 20 seconds there, maybe 30 seconds to go through it. We can give them maybe a solid 30, I think that's fair. Okay. And just in your case, to your best, you can see that this is a pretty wild environment as Luke is actually alluded to at the top of the call. So this is a live implementation of the open telemetry Oltel demo. So this runs that same set of services that are distributed. And so the only thing that's always true every single time we run this hands on is that Kafka is like a totally memory hog. But that's in a lot of work, that's okay. What I will say is that some things might jump around or ever. Do your best to answer the question, we'll still give you the certificate at the end of the call. So right on. Okay, well, we're gonna get 20 more seconds folks. And so as Luke has saluted to earlier, or said earlier, sort of what are the three containers with the highest memory usage? Also, another fun fact using, so we have basically we're connected to the head of the actual get repo for the demo. And so sometimes the stuff just changes because we'll just do a pull and deploy, which is obviously like a cardinal sin in the world of DevOps, but you know, YOLO I guess. So cool, I'm gonna have a call. And you walk through the answer. Awesome, awesome. So a couple of different ways once again, everyone, we probably could have arrived at this. I see some great answers here in Kafka is in both them. So that adds up. First things first, let's make sure we namespace the Oto demo, right? As we talked about before, run that query, so now we're just in Oto demo. And now we could, first of all, we could do this in a couple of different ways. Just for example, brute forcing by looking at it and try to find it. Notice that the values do change. So depending on your time frame, you could have one answer, you could have another time frame. But only that we could have gotten at that is just look at a sort of breakdown of the sum by container, right, over the last, let's see, four hours here. And that would have given us Kafka, add service, load generator, which it doesn't count, it's just generating load. But then any one of those other services, right, would have been an acceptable answer here. The whole point that we were trying to get to is just make sure that you're doing a sum by containers, right? And sort of aggregating it by what you're looking for to give you that answer. Well, one thing that I'll sort of mention here, going back to our troubleshooting scenario, is that it might be hard to see, but a recommendation service is not doing too hot, right? It's a little squirrely compared to the other metrics. So one of the things that we can do, it's sort of just dive into it, right? We can click that recommendation service name, we can click on the time series from our recommendation service, might not be as easy as it looks. So let me just filter this out, recommendation service. There we go. I should make it a lot easier for us. There it is, right? It's looking a little squirrelier. You can have done this by clicking on the time series, it's just a lot of data there. So it becomes a little harder because it's filtered out a little bit more. But what I want to do with this now is do things. I think that it'll be worth adding this to our dashboard just so we can keep all of our sort of investigation steps in one place, right? So similar to what we did before with the query value for our number of warnings, we can add this to a dashboard just by clicking on the top right hand side of your screen, right? In actions and then add to dashboard recommendation service to or whatever you named your dashboard in that case. And then if we go back to it and refresh it, there it is. We can see that our new time series is right here, beautiful in place for our investigation. Awesome. Now let's take advantage of what we have here already so far. And let's create a monitor off of it, right? I mean, just going back to the troubleshooting scenario. The only reason we knew things were going wrong is because our customers were letting us know. So let's remedy that right now at Hawk and create a monitor so that next time we know that this is going on, we doubt the customer being affected for a long time prior. So one of the things that we could do is once again, instead of adding this to a dashboard, it's just creating a monitor for it, right? And once again, observe is going to take the context of the time series that you're looking for and then throw it into a monitor for you so you don't have to refine that context, right? Everything carries across your views. So this is the monitor creation page. Just to give you a little lay of the land, we have a time series of that metric over however many hours you're looking back. We have the query for that metric, which we saw before. So we're namespace, Altademo, recommendation service container. It's a sum of its metric averaged over 30 seconds. And then we can create our alert conditions, right? So everything's a nice little, we have a nice little gooey here for everything. We can choose the status or I guess the severity of this monitor whether informational or important. We can change the type of thresholding that we do, whether it be when the metric is equal to a threshold, greater than a threshold, less than a threshold. We can change how we evaluate when it goes over that threshold. And then how long for it to be above that value, right? When we alert you. One thing that I'll mention here is that while we're looking at a static threshold in this case, we can, it's more than possible. It's expected that you can create dynamic monitors, right? Based on a specific metric just by using things like a moving average or a standard deviation, right? And we can create those sort of more dynamic thresholding monitors for you. For the sake of simplicity, let's just keep this one static. And let's choose a decent threshold for us. So we can see here, you know, memory rarely, rarely goes over this sort of 40 megabytes range. So let's put our threshold at around that value. The thing is that since we're going to be dealing with, I believe just bite zero, we're going to have to add quite a couple of zeros, but this should take us there. Right about 40,000 or 40 million? Yeah, 40 million bites should take us there. It's up at about 30 megabits. Seems like, but we see that threshold being generated there. And one of the things that I'll do here is change the type of thresholding just to show sort of a different representation of what the monitor would look like. Right? So in this case, instead of having to stay above that threshold, I want to be alerted when my match across is at threshold at least once. And one of the things that observe we'll do is highlight for you in that time series when the monitor was going to trigger, as well as show a preview of when this alert would have happened as well. We could also add different notification actions for this. Right, we can go ahead and throw a webhook to your favorite communication collaboration tool, create a gerothaic, whatever you want here. Webhook's quite flexible. But once we have this in place, let's go ahead and save it. Oh, seems like I already have a threshold monitor in place, which is good. That's a pretty generic name. Let's go ahead and call our recommendation service, our RICO service monitor. Hopefully that one doesn't exist yet. There we go. Right? So now we have a RICO service monitor in place. Kyle, I saw you throw you threw on a quiz question here. I, that was my fault. Yeah, sorry. I forgot to close the last one because it moved the closed screen. So no worries. No worries. You just want to make sure I wasn't missing anything there. No, no. Yeah. Cool, everyone. So we've got our monitor in place. So if you want to take a look at it, let's just hop over some of the monitor stuff. And then we should see there actually I had a bit of monitors for this. But the latest one is just RICO monitor, May 9th, right? And you should be able to see something similar. We can go ahead and I believe click on it as well. Maybe not here. Anyways, there's our monitor, right? We can view the alert. There is view the alert. And here we give you sort of all the context behind that. But awesome. Cool. Let's take a step back here. We've fixed this problem of not having a monitor that is. We have a dashboard. We have our monitor in place. We'll be alerted next time. So now that we've investigated logs and metrics, I think it's only fair that we move on to traces to see what the customer experience might currently be feeling like for our customers, right? And no better way to do that than to look at the requests that they're making to our website and how long they're having to wait to receive an answer to them. So if we pivot over to our traces explorer, it should be fairly similar to what we saw before. Make sure if you have no data here that you're looking at the demo data slash Otoll data set. I was playing around with some open telemetry earlier today and I was starting data into the sandbox. That's why I wasn't the other one. But you should be defaulted to this one. So I'm not too worried there. So once you've navigated to traces, let's expand our search to the last four hours once again, just to get some really good data to our platform here and within view. And by default, we're filtering out your spans to service entry points, right? Meaning the first call to a specific endpoint, not necessarily the subsequent calls made from there. Just to facilitate, just to facilitate to me, if you're curious, you can go ahead and remove that filter. It's going to be a lot more data in your screen. But by default, we're sort of scoping to that. What we can do now, though, is quickly look at all of our error traces, right? They click of a button, filter out to only response status of errors. And then we see all of those calls. And there's actually quite a bit of them. Quite a bit of errors going on here. We'll take a step back here, though. Take a little break for another quiz question. Let me think of any context that I can give to help you guys before we dive into it. And now this one is easy enough. There's a couple of ways to get there. It's going to be a challenge for you guys. So the quiz is how many? Question three, right? How many? And what are the names of the services that we have error traces for? So notice that we're already sculpted into response status of errors. Notice that there's a couple of filters here for services. I'm wondering, is there one filter here that would help them Kyle get there? Should we be scoping into the namespace of demo data or hotel demo you think? You know, we can do that. There's a few different sort of quick paths. And I think what we'll do is let's give them 20 seconds. I think you give some very, very ample hints. So I appreciate that. Okay. And then we can pivot from there. So what we got here. Okay. All right, nice. Nice. There might be a subtle trick to this one. So we'll see. Yes. So that said, let's have you take it away. I'll end the poll. All right. Beautiful. Okay. Okay. Okay. All right. Beautiful. Okay. Couple of different ways we could have gone about this. Right. Everyone. So for one, just by clicking on this little filter section here, we would be able to see all of the services that we have errors for, which is awesome. Right. It's already filtered out to that. A couple of other ways we could have gone about it is by looking at the filters on the left and side. Or even if you're feeling very adventurous, write some opal for it. Now, I'm not going to do that here. But it is one of the ways that we could have gone about it. Just doing a little mate column to create a table view or a time series. Any whatever floats your boat there. You're feeling adventures. But the next question is sort of tied into the same sort of context here. Which service out of those four or five in this case. Have the highest number of client errors. Error traces in this case. I'll give you guys another 30 seconds or so 40 seconds to get that one out. I'm going to give you a couple of ways to get that one out. There's a couple of ways you could go about this. You could brute force it. You could find something that I haven't shown you yet. Or just. At times series bar charts. Or we want to. We want to do. Nice answer so far. There's certainly. The least wrong. For sure. I also, you know, I feel bad. I've most front end teams. If you have sorry apologies to folks that are back. You work hard to you. But most of the time right. The way that your customers interact with whatever services you provide is through the. And so usually that's where the they propagate up. But it's not always front and fault. I've actually argued it's very rare. That's really the case. But. For friend and folks who like. You know coming at this display and like you're the biggest slice of pie on the pie chart. Feel a little bit bad. So. It seems pretty targeted. Kyle. Do you have some some previous beef with front end folk? No, no, no, no, no, as somebody who's incapable of writing any sort of quality front end code. I think I think they get blamed on duty. All right. Cool. So answers are in. Let's say let's take a look at this. Let's take them through a way to get there. Yeah. For sure. So folks couple of different ways. Right. For one, we could probably just bet an eye on this and see that a lot of them are coming from front end. Right. So. On the other hand, we could try to use the filtering approach and observe will always show you sort of a little barge showing just how many of each value is found within the column. But the best way I would say I guess the most correct way would be to group this little sort of you have we have some red metric bar charts here right requests errors duration. We could group that by a specific service group the error times here is by specific service and we'll see that by far in large the largest shares coming from my front end service. Right. So once again, just a couple of different ways we could have arrived at that issue, you know, observe gives you. Plenty, plenty of different ways to arrive at the same same result. That being said, going back to our troubleshooting scenario, we can immediately tell that our customers are just not having a great time right just given the sheer number of errors coming from front end. That said, let's take a step back and since we didn't see any errors being emitted from recommendation service, which is weird, right. I mean, it's what we were seeing on our container logs. What else might be going on here? So let's do this. Let's scroll out to the attributes column should be here towards the end. So if you just scroll in our table view all the way down to the attributes. One of the things that observe is quite powerful about and honest one of my favorite features is the ability to manipulate your data within the platform. So we talked about the sort of the data liqueur texture and being able to receive any sort of data whether structured unstructured. Here's a great display of how we use that power to allow you to sort of transform it post ingest, right. So scrolling over attributes here, if you click on the header of this column, we can see that it's a bunch of Jason, Jason objects. We can extract from Jason, right. So what observe will do for you is it will look at that object, right. Find all of the attributes within the object and tell, hey, what what do you want to bring into view here? And that's what we call schema on demand. So in this case, what I want to bring into view is my HTTP target attribute. So if I select that and then I click apply, it'll do what it will do is schema on demand will take that attribute and on the fly modify the schema of my table to introduce us that new column. That's quite helpful for a certain fight which endpoints are being hurt by this. So now that we're here, I'll give you guys about 10 seconds. I'll give you guys about 10 seconds here for us to catch up, but we have our fifth quiz question. So 10. Cool, here's the fifth quiz question. So once we have this attribute in place, the question is based on this HP target field, what four services are being impacted. Right. So using this newly related field that we just brought into view, which four services are being impacted. Here's about 30 30 seconds to get through it. Yeah, one of the favorite things that we did recently is this can sound super nominal, but because we talked about front end earlier. If you scroll left and right on a newly extracted field on a column will actually read highlight it for you. So sometimes it gets a little bit dicey China member like K, I just added a new column or a moot it or whatever. And so we can we get a blue dot, of course, too, but there's a usually nice little sparkle effect. We can be layered on top of that reason. So today I learned. That's right. All right. Ooh, a variety of variety of answers. I like this. All right. Let's let's take them through it. They can through it. Yep. Cool. So everyone once again, couple of different ways we could have gone about this. I, I, in my opinion, the easiest one would be sort of just about an eye on it. If you do a filtering approach. And we can see right we have a products endpoint. We have recommendations endpoint, I'm a checkout endpoint. And then all the way down, you have a data endpoint, right, which I'm not quite sure what that's doing, but it's there. It has errors attached to us. So it's probably investigated at some point, but that can go in the backlog. So if you said checkout data product catalog and recommendation, you were correct. So good job there. So what we'll do here now. Let's filter into our. Sorry, the question when the cuspile back up. Let's just filter into our recommendation, sort of endpoint here, right, because you know, it's generating front and errors or it should be. By people making calls onto it. So let's apply this, this filtering here. Let's take a look at all of the. The HB targets that are just for API slash recommendations. What we can do here now, so that we're triggered, we're not triggered. We're scoped into my recommendations endpoint. Let's go all the way to the left. And it allows us to. Oh, it actually, we don't have to go all the way to the left. I didn't notice this. But the view trace button is frozen in time so that we can dive into whatever row we find. We can dive into the underlying trace view for that, right. And you should, you know, you could be familiar with trace views, but it should give you guys a little land here. It's just going to be a flame graph of the HP call stack. Right. So someone made a request to that first endpoint that we saw, which then called on to my recommendation service. And I think that that observable do for you is highlight the, the spans that we have errors for. Right. As well as bring into view, hopefully. The stack trace, where do my stack trace go? Span events maybe. Yeah, it's also possible. There we are. Hey, yeah. So just a quick warning guys. If I'm span events as zero click on it anyways, because it'll update to show if there's any errors that I should probably. Don't think I look at this guy later on. But one of the things that the tracing will do, which is awesome is it will highlight where you have errors, but also give you the metadata behind that error, right, which in this case, if we double click it. Is the stack trace of that problem, right. So we can see that that sort of spike in memory that we saw in Mexico explorer. It's very likely triggering an out of memory, right, where we can connect to my back and anymore. Does killing the service and sort of making it available for the front end. That's why we have all those front end errors. Right. So our front end is trying to call our back end. It's not able to and then it comes back with an error, which looks like a front end error. So we look like we can at this point, you know, pretty much fully confirm the user reported issues. What we need to do at this point is probably just page out the team that owns the recommendation service, come up with effects. And then in the meantime, we can sort of bump up the memory limits from my back infrastructure so that it stops going out of range of arts failing from what I remember about 80 megabytes to do the job there. The thing though is that if we hop back into our explorer, right, if we click on this little back button and get back to our traces. It seems that there's additional errors that we're seeing across a few different services here. Now we can sort of continue through this trace explorer to get a better picture of what is going on, right. So close that flame graph view. Let's remove this recommendation service H to be target because we're already done with it. We've confirmed that that's something going on there. So let's get out of there. What I want you guys to do is to use one of these filters to look at a minimum duration of your traces about 15 milliseconds. Right, so on the left on sort of the filter view, go ahead and do a minimum of 15 milliseconds first to look at those slow calls or not that 15 milliseconds is slow, but this is for a sake of the story here. So once we're filtered into all of the traces that are above 15 milliseconds duration. We can see that there's still recommendation issues here, but it looks like our check out service is being uniquely impacted as well. Right, and that's not great because that directly impacts our revenue as an e-commerce application. Right. So let's dig into one of these traces right right here. Anything that says status code 13 should be pretty good. Let's dig into one of these to see what that 13 is all about. So we can see that this trace is a little bit more complete than the others. We have more calls. We have more context, which is great. You guys can explore that if you want. I think it's an awesome little view here. But in this case, what to quickly reach into the root cause. We can simply just click on the first occurrence of an error. And I love this right. We can just click on the first occurrence of an error within my flame graph view. And then observe is going to go ahead and highlight that for you of what that error is right now. In this case, it just turns out that we had a feature flag to go ahead and fail. But just like a real deployment right, it might seem like a pre canned process, but just like a real deployment. I mean, you start off seeing front and errors. You go into your traces. You find something weird. You hop into that trace. And then just by looking at the first occurrence of the error in your back end. You'll be able to get a pretty good idea of what's going on, whether it be an error message or the stack trace right at that point in time. Now we can always disable this feature ourselves right to remove one error path for our users. But that sort of concludes the first part first remote shooting part of our of our flow here. We're going to shift gears a little bit and we're going to go towards sort of what we call our first challenge right. And for that, we're going to hop back into our logs explorer view. Still using our container logs demo data set right container logs that we're using that source of data. What we'll do at this point is let's go ahead and filter into our controller application. Sorry, controller container. So with the click of a button, you can get there. And let's pretend for a moment that we never had a PM in place, right, because the way we found their error in the last problem, the way we sort of got so we got was by looking at our traces, you know, not all I'm not all teams out there are fully fleshed out with their. The line machine in terms of a PM. So how could we arrive at that sort of same sort of context just via our logs right in the skays, ingress logs for from from our observer in the skays engine next. We should get pretty far with just as information by taking advantage of what I talked about before, which was schema on on demand is schema on read right. On the log column header. If we look at this are sort of web service web server logs. What we can do is we click on the column header. Is to what we call an extract from strain right. So unlike perhaps other observer solutions in the market, we don't have to create a pipeline for logs. We can use the schema on read functionality to go ahead and modify once again what we're looking at with the information that's within my logs, which once again doesn't matter how you send into us, we're sitting on a data lake, we can play around with them. So what we'll do in this case. We're going to write at redgex to go ahead and extract the information from here. So if we click on this generate redgex button, Kyle is going to go ahead and paste the redgex for you guys in the chat just in case, you know, we can get it done via the the oligipity assistant here, which actually should talk about right. The bt is sort of the baked in gen ai that we have with an observe where it will act is not only your ai assistant, but it'll also help you with sort of these menial tasks is certainly non trivial, but it'll help you with generating redgex, generating opal right and just assisting where it can. So once again Kyle posted in the the redgex into the chat just in case. So there's also a reason this button doesn't work, which is should, but once I click apply, what it'll do is it'll extract as much information from my log as possible can right things like the address the data, the call, the method of the request right the URL that we're hitting here. Anything a number of new columns there, which were highlighted in green as Kyle pointed out later earlier on. Now. The two most interesting columns for me here, which sort of give me a sense of application monitoring are two is the URL that we're hitting right the end point they're hitting here and then the status code. So one of the things that I want to do is I want to extract some information from my URL by doing the same approach that we did before and performing extract from strength. Once again, Kyle is going to go ahead and post the the redgex for us into the chat. The biggest difference here is that, I wonder if that'll work, let's click apply. Let's check it out. Yeah, so the redgex redgexing a redgex, which is kind of, you know, I just find that to be totally well. The other thing I just noticed is it looks like somebody's using it. Some sort of scanning bot, I think, based off of the URL. Oh, there we go. Perfect. So I would highly recommend using the backup, the backup redgex I pasted in the chat, but functionally, if you look at the URL that's in there, that's not sort of what I call traditional application traffic, which, you know, the great downfall of rejecting your redgex is when someone inserts. Absolutely. And the whole thing, you know, using LLMs is it's incredible. If we were to just scope the data into a little bit more similar endpoints here, it'll work perfectly, right? It's just that we're looking at across a lot of different context here. You can see there's a couple of different types of URLs, or it does its best. And you could probably build off of what it gave you very easily, but in this case, or we have the pre-built redgex here, why not just use it. And then we'll see it back to the story. We now have a new API endpoint, sort of column, so data. And then we also have a status code next to it. If you put two and two together, that gives us a pretty good understanding of the performance of each one of my endpoints that I can then create a metric to alert you on top of. So essentially what we're doing here is the arriving application related metrics just via my logs, right? What we can do is let's go ahead and aggregate on these status codes, including the no ones. By simply clicking start a visualize button and this little visualize the chart button in this case. So what it's what it's doing now is it's doing a kind of values, which is fine. Kind of all events, but what I want to do is I want to break this down by your L first so we can run that just see what it looks like. Pretty messy. But then we can go ahead and add status code run that again. So what we're doing now we're effectively visualizes a bar chart just for the sake of simplicity or beautiful. We're effectively looking at the performance of each one of my endpoints as a metric. And one of the things that I'll highlight is that first we have an instrument that our application and second, I'm adding two pieces of card now to this metric, which if you're using most observable solutions in the market today. Right, this will be considered 84 custom metrics with an observe. This is just data right there's no extra charge for what we're doing here business just a metric to your deriving and it's just present within your logs. So we're extracting that generating this for you and no extra cost. Me as someone, you know, just a side side point here. Someone who comes from data dog. Great product, but the custom metrics there are scary scary scary come complex. This is very simple. No bag is associated with it. Love it. Also, if you want to show them so obviously one of the things you did is you extract the status code as part of the initial rejects if you if you swap your L out for I think we call it API. I think maybe it's API end point good call. So slicing and dicing the day. It can be a sort of complex or as simple as you want it to be in rejects is like a classic tool that everybody I'm sure on this call has had to use one point or another. So we're doing the Lord's work trying to make that a little less onerous. I still, however, will continue to have bookmarked rejects 101.com for testing. I'm sure one day the AI will over Lord's will figure it out, but today is not that day. Yeah, it'll get there at some point and look, it's already made a lot easier than used to be in say five years ago. 100% 100% Awesome. If we want we could also just as an aside here, we could change the time. The aggregate the time that we were the resolution is what I want to say, right? So that instead of aggregating this every 30 seconds. I might be more helpful to aggregate this, you know, perhaps every five seconds or every minute. It's completely up to you. We just give that flexibility within the platform itself to look at your metrics or as granular as as you want. So just a recap, right? While having full spans and traces is optimal, we can actually take something as simple as an ingress log from your web server. Just see how our services are doing and then go ahead and alert on top of this. Right, so we've already, you know, confirmed that there are some serious health issues within our recommendation service. But if we didn't have that data in in APM and we were in a pinch, observe can actually just take commonplace data sources like ingress logs and transform it into something that's pretty close to APM. Now that we have this view into place, we're going to go into our quiz question six number six. It should be which APIs are throwing a 308 status code. Right. So once again, which APIs are throwing a 308 status code? Just as a hint, the answer is in front of us, but there is ways that we could make it a little bit more obvious just by playing around with the visualization. Absolutely. And because we took the time to break out our status code as a distinct filter, robo value, you can even look at games, something like that. So I will give you all 30 seconds or so to answer this question. Something else that you kind of touched on, Lucas, and I think it's really important just to talk from our experience and working with a number of customers, especially in the last year is. There is a cost piece, as you mentioned, and so because we treat everything functionally as just sort of data that you can bend to your will and we obviously have first cost support for the three pillars of observability logs are ubiquitous right and to go from logs to distributed tracing is a journey like customers were either at point zero or maybe they're 100% there. I don't see that super common, but it's really, really we believe important to say, hey, let's let's take the data you have and let's build something that everybody in your organization can benefit from. Whether or not you're you've done every that you've auto instrument everything to hotel or we spent that time that that's just that's hard that's additional work on top of you know delivering actual new features to the services you already own and maintain so. This is why we do this this exercise is just to show everybody like what there's actually a way to get a tremendous amount of value at a stuff that's just kind of sitting in the background is a sort of digital exhaust so. Alright let's end that poll and let's show them that shortcut let's see how it goes. From the shortcut oh which shortcut ah ok so one of the things that we could do is by. Wait one second i'm trying to what was a shortcut now if I was yeah if you if you move sorry you're good if you move status code into the filter bar so where it says contain opposed controller if you need a status code equals. Yeah that was missing something for a second but yeah everyone one simple thing that we could do right is that we know we're looking for a three like status code so we could just do it search on top of it. And then it would just highlight all of the end points that have a three like status code the other way we could have arrived at that which is simply be by looking you probably how couple of us arrived at it is just by looking we could by an island is an NC word over at. I would review some other visualization types like looking at a table list right couple of different ways we could have right that it. Last single highlight is once again all of our rejects all of our visualizations all of our filtering is transformed into all pull in the back in here so for everyone who said all of the above that was correct good job. So well we're at the end of our troubleshooting process so congratulations we've started successfully navigated our scenario and we discovered a two of our services we're causing customer issues and now we have a path to resolution. We do have one last exercise as I alluded to before and this is very you know there's something that's very much unique to observe approach to observability right we're going to do what we call data shaping and turn some of our observability logs into business insights and it's a lot of fun and product managers like Kyle love. This functionality now this will touch on a number of new concepts so feel free to ask questions in the chat if you get stuck if you need help understanding something better clarification and so on. So let's hop into our bonus challenge of using worksheets now understanding things at the service layer is essential to getting ahead of as well as fixing problems right. However, anyone who's been in charge of sort of ensuring up time for a service whether it be monolithic distributed otherwise knows one of the hardest sections to answer during let's say a post incidents review a post warrant is the assessment slash evaluation section right particularly understanding if you know from from what you have probably had available to you during the incident was that adequate to solving the issue. Now rather than starting in our explorers we're going to go to our data sets page and we're actually going to know our hands started with some pretty pretty good opal. So if we hop over to data sets. We're going to be looking for our sort of base data set for the demo data namespace right so if we just you know once again navigate to data sets on the left hand side of your screen and then scope into demo data and make sure we do demo data name I forget the differentiation here Kyle I remember you. Explain that once but I forget as to the difference between these. Yeah, so yeah go ahead I was going to say so the way that we sort of sort of sort things and namespace them there's a special namespace called no package and that's the one we want so when you when you see no package and observe. Think of every data set that's not packaged almost always going to be tied to that data stream that we just talked about in the architecture slides earlier. Think of it this way we're dumping observability events into the central location that's what called data stream and then you can create data sets off of that but every data stream has a sort of a route data set better base data sets what we actually call. And so that's that's why we say look for no package it's so you don't get into a pre pre shaped piece of data that's all. Makes sense makes sense awesome so so now they were here we sculpt into it don't don't click on the data set itself. On the all the way to the right and time of your screen there's going to be an option to open this in a worksheet and a worksheet is sort of like a playground right here send box when dealing with any sort of data sets within observe whether it be a no package data set is explained or in a very populated. Already transform data set so once you open this in a worksheet you should be taken to a view like this where you have the once again your tabular data we have your query builder top right you know top of your screen. But then we also have an opal section that we can write opal with in sort of our in page ID if you may to write writing some opal. Now as Kaya mentioned there's going to be quite a bit of opal that we wrote here so he posted a link in the chat that takes it to sort of that pre build opal what i'm going to do with you guys is i'm going to go line by line on this opal and i'm going to showcase how it's transforming the data just make sure I get the right information here. So once again go ahead and click on the link in the chat it should be a tiny URL with the opal pre written there for you the scorpion based into your opal sandbox hero your opal window and then we're going to go line by line understanding what this is doing to your data and how it's shaping it. So again if I run this nothing should change I have my sort of raw data there what i'm going to do is I'm going to go back to 24 hours just make sure I have as much data as possible and then we're going to go line by line right so what the first line of opal is doing it's creating a column named tag and it's. filling it up with information that's within my extra column right so in my extra object the tag attribute is being transformed into a tag column so i'm going to go ahead and run that. So we have is a new column tag because it is all no but that's okay it's just because that there's so much data within here. And what I want to do is I need to go ahead and fetch that data from all of us so there should be with almost 70 million rows of data here we can say that once I fetch from everything then that product catalog shows up so there we have it. So what we'll do is we're going to go ahead and filter specifically into that product catalog right so filter tag equals product catalog we can see that these. roles are coming our interview as observe is sort of parsing those 70 million rows very fast and about seven seconds to. Well then do is create a new column for the product products which is based off of a fields object. I should say a fields column within a products object and then we have it right our new product column it's here one thing that I noticed is that it's currently an array right which honestly goes back to what we talked about before it doesn't matter how you send the data to us we can shape. fit after the fact where so we even have an array of JSON within observe and x honestly pretty cool in the opinion what we need to do though is flatten that our rates we can then access the objects within it and transform each one of those into. A column itself or a roll itself I should say right so now we have all that into. Finally we're going to take all of the information that we just went through and we're going to parse it into different columns so that we can access them a little bit easier. Right so within all those J songs within that array what my opal is doing here is it's accessing them since we just flattened it and then creating new columns based off of the attributes within those JSON. Then we're going to do some operation some operating on one specific column which is price where we're going to have to add two values units and then it's just being sent in this case we're going to make them into a string concatenate them that's a social the part of opal here concatenate them turn them back into a float and then round them to the nearest set. So what we should be able to see here is the price of each product that we just arrived from two strings. Finally we really want to magic happens in this case is we're going to take all of this data and we're going to transform it into a resource data set so we can track the state of my products across time things like changes in price changes in description. I were basically going to be creating a product catalog. In this sort of format here we have the product name we have the price the product the currency to end the categories associated to in the description of it beautiful finally we're going to set a primary key or I should say a label to this product data set which is based off of the name all of this does everyone is that when I reference this data set within a different one this is what it's going to bring into view it's going to be the name of the product. So it's creating the label is based off the name column so with all that opal right just to recap what we did we went from the raw data which was this into an awesome resource data set that showcases all of my products and it's tracking the state of them across time. I just want to say again as product manager really loving this. Awesome you came up with it right so I hope so. So everyone once this once this this opal successfully runs once we have something like this go ahead and rename this query right some of the pop life inside of your screen you can see as we do new queries it's going to create sort of new worksheets or new windows into your worksheets. Let's rename it. I'm going to rename it what was it called astroproducts it's probably going to give me an error because I definitely have an astroproducts already so I'm going to call it astroproducts five. Beautiful if we want we can go ahead and save that worksheet I recommend doing so right with saved it but then what we can also do it's published this data set right as a new data set. So what this will do is just make a permanent so other people can access and reference to it and relate back to it so once I publish it I'll rename it to astroproducts again so we can identify it. See I'm going to call astroproducts five just make sure they're there click publish what that'll do is to make that data set that view that we just created permanent so we can reference it back. So with our new data set created let's go back to our data set view. Let's search for astroproducts five notice that there's one two and five now and there we have it right it's currently initializing observe is sort of spinning up its gears is warming up the engine to create this resource data set for us. For the next step though we're not going to stop there let's go ahead and look for container logs sculpt into demo data so search for container logs make sure we're using the demo data package one not the Kubernetes one we've made some changes we can see this one action has data in it to and let's similarly open it into a worksheet view just like we did. Now what we're going to do is we're going to predominantly use the the UI to build the open here just to make sure that we're used to that to and just make things a little bit quicker. First things first let's go ahead and filter into my cart service container. So go ahead and stick out the container header make sure we're looking at cart service click apply it's going to bring in the filter into view for us here. Now once we're here we want to extract some information I guess you guys can kind of see where we're going with this right we created a product catalog and our look at our cards let's hop into the log message and let's do an extract from string. But instead of using rejects this time we're going to do one of the use of one of the pre built parts is that we have for key value pairs so we're going to try to extract some some some key value press from this so once again column header view extract from string. Extract from string key value pairs like apply observe will do its best to go ahead and try to extract any key value president identify identifies from your log messages. Building into that new sort of KV pairs column. Now we're going to do one more extraction here but this time we're going to take advantage of the JSON structure or new KV pairs column right so once again go into the header do an extract from JSON and let's extract all three fields from that column and bring them into view for us here. Alright so there we have it product ID quantity of products and another user ID as well which is really nice because we can now tie back in how much each customer is adding to their car in terms of product value right because we have that information now. Just to highlight that again the opal is built for us via the UI actions so that's super helpful. And now that we have our three new columns here what we're going to do is we're going to use the power of graph link the correlation observed creates across everything to correlate this data set to our astro products data set that we had before so on the ellipses here on the top left inside of your screen. Go ahead and click on link to other data set. Well that'll do is it'll open up this nice little gooey first to choose which data set we want to link this to and I'm going to link it to astro products five as we just created. And we're going to link this via the column of product ID right because notice that we have a product ID interview and we have a product ID in my. My astro products data set so we're going to link into this and then the link name is just going to be astro products I just first know what's going on there as I click apply. Remember how we set up the name of the product as that initial label so that's what we're bringing to view we bring in a new correlated column with the name of the product right and it's in green me that there's a correlation back in there so we've done is we've just joined our new our first data set. And notice that once again the default name is coming in as a label but what I could do is I can rad related fields from that old data set that we just had things like the price right things like the name the categories description and can bring everything into view here and we can sort of apply and once again use scheme on the man to modify this schema of my once again logs you is what we started with. Now one more thing that i'll sort of point out here is that observe. Hipes all of its data so if I want to take something like let's say price. And it's a string I need to make sure that I modify it or change converted to an int right before I can go ahead and and build a visualization topic it. And then once again sort of writes the opal for us everything in the backend. So worksheets have us like different flow to creating visualizations then the next floor is due but still we can click this chart button here and it'll give us sort of a count of how many products are added or how many products are logs we've seen what we want to do is just do a quick sum. Or is it some of astroproducts price by astroproducts and hopefully what that will give us. Is how much we're getting paid. By each product right and someone like Kyle would love this because then he had they have some business insights as to where the focus or efforts on it seems like optical tube assembly is. Super super proper big fan big fan yeah optics of any kind yes I notice I might have used the astroproducts price when I should have used the currency or the valley I don't know in any case we seem like we're making a lot of money off of this and it's the same end goal. That's right now you know just to sort of save this for later use we can just add this to the dashboard to be greater earlier right I can nation service to here bring that into view and if I refresh this. We should be able to see that third beautiful bar chart that we just created to keep track of you know how much money we're making off of each product awesome. Perfect bonus question and I guess our last question here is going back to this worksheet right here. Which product by name do we have a question for this one Kyle prebuild or this is just a bonus one for them to figure out. Yeah it's about so you don't have to answer your via zoom they just want them to think about it just really feel it out. Yeah yeah we'll challenge. Alright so the question third question is everyone which product by name has the highest number of add to cart actions in the last four hours so not price actions you know how many of them have been what's the product has been added the most think of it as your PM giving some homework to figure out more business insights. Awesome I think we're done or done here right we've covered what let's just recap what we covered we went through a troubleshooting workflow to figure out what was wrong with our recommendation service within our front end and in our a commerce application and then as a little bonus we found out that the other problems you know within some other services that we were able to remedy there. Then lastly we went through some opal exercises we built some really nice visualizations here and added them to a dashboard. And we hopefully learned a lot along the way of observing the powerful plan from the we've created here so. Just to sort of finish some housekeeping items here everyone first of all thank you I thank you for joining us through today's workshop you're going to get a survey probably from grant to give us some feedback on this so it'd be awesome if we could get some insights there. The trial that we just created is going to be yours for you to use for the next 14 days so for for to play around with it add some data your own data if you want just explore as much as you want. If you're interested in a data shaping session reach out to schedule some time with grant and please feel free to join us for our next webinar with our one and only keep bus well one of our other sales engineers here within observe is going to be a workshop on navigating regulatory compliance with observability in modern approach on May 16th. I enjoyed this one you love that but that I'll pass it back to you guys Kyle and Grant. Yeah so just for me thanks everybody for signing up we're actually using observed a watch folks sign up because we've actually instrumented the provisioning code so it's always exciting to see folks log in so thank you for taking time today I know time is precious right so. Really sincerely appreciate folks just going through this and Lucas obviously for doing such a great job and I will do a shout out as a product manager of a sin product years ago regulatory compliance near and near to my heart probably the nerdiest thing I've ever said that it's actually pretty good on so if you have colleagues on the cybersecurity side of your teams encourage them to check this out we actually have some pretty interesting stuff to offer on that front so. And Grant if you want to close us out by man yeah thank you so much Lucas and Kyle i'll have everyone's observability innovators certificates out no later than tomorrow and and hope to see everybody on our upcoming webinar thank you so much for running this Lucas and Kyle and I appreciate everybody's time today. Thanks everybody take care have a great one take care.